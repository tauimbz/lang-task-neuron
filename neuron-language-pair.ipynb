{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10856395,"sourceType":"datasetVersion","datasetId":6743330},{"sourceId":10868887,"sourceType":"datasetVersion","datasetId":6752292}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:00:48.033882Z","iopub.execute_input":"2025-02-27T00:00:48.034256Z","iopub.status.idle":"2025-02-27T00:00:54.695017Z","shell.execute_reply.started":"2025-02-27T00:00:48.034217Z","shell.execute_reply":"2025-02-27T00:00:54.693397Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### codes are fixed for InferenceModel, all its models, and Visualization Per Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\nfrom datasets import load_dataset\nimport evaluate\nimport pandas as pd\nimport torch\ntorch.manual_seed(42)\ntorch.cuda.manual_seed_all(42)\nset_seed(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nclass InferenceModel:\n    def __init__(self, model_name, device):\n        self.model_name = model_name\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", offload_buffers=True)\n        self.acc_metric = evaluate.load(\"accuracy\")  \n        self.precision_metric = evaluate.load(\"precision\")  \n        self.recall_metric = evaluate.load(\"recall\") \n        \n        self.df = pd.DataFrame(columns=['pred_answer', 'pred_answertext'])\n        self.predictions = []\n        self.references = []\n        self.answers = []\n        self.eval_results = []\n        self.model.eval()\n        self.logits = []\n    def init_eval(self):\n        self.acc_metric = evaluate.load(\"accuracy\")  \n        self.precision_metric = evaluate.load(\"precision\")  \n        self.recall_metric = evaluate.load(\"recall\") \n        self.df = pd.DataFrame(columns=['pred_answer', 'pred_answertext'])\n        self.predictions = []\n        self.references = []\n        self.answers = []\n        self.eval_results = []\n        \n    def get_logprobs(self, prompt):\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        outputs = self.model(**inputs, output_hidden_states=True)\n        logits = outputs.logits\n        # Shape remains, but values are now normalized as log probabilities over the vocabulary.\n        logprobs = torch.gather(F.log_softmax(logits, dim=2), 2, output_ids.unsqueeze(2))\n        return logprobs, outputs\n        \n    def inference(self, prompt, max_new_tokens=1):\n        \"\"\"\n        Performs inference on a given prompt.\n        Returns the decodede output\n        \"\"\"\n        generated_text = ''\n        len_sentence = 0\n        if self.model_name == 'Qwen/Qwen2.5-0.5B-Instruct':\n            # WARNING: messages ini harus sama dengan messages di check_index_prompt\n            messages = [\n            {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": prompt}\n            ]\n            text = self.tokenizer.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True\n            )\n            model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n            \n            generated_ids = self.model.generate(\n                    **model_inputs,\n                    do_sample=False,\n                    temperature=None,\n                    top_p=None,\n                    top_k=None,\n                    max_new_tokens=max_new_tokens\n                )\n            generated_ids = [\n                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n            ]\n            generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n            len_sentence = len(self.tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)[\"input_ids\"])\n        elif self.model_name == \"google/gemma-2-2b-it\":\n            # WARNING: messages ini harus sama dengan messages di check_index_prompt\n            messages = [\n            {\"role\": \"user\", \"content\": prompt}\n            ]\n            text = self.tokenizer.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True\n            )\n            model_inputs = self.tokenizer([text], return_tensors=\"pt\", add_special_tokens=False).to(self.model.device)\n            # print(f\"(model_inputs['input_ids'] {(model_inputs['input_ids'])}\")\n            \n            # print(f\"len(model_inputs['input_ids'] {len(model_inputs['input_ids'])}\")\n            sentence_token_texts = self.tokenizer.convert_ids_to_tokens((model_inputs['input_ids'])[0])\n            # print(f\"sentence_token_texts {sentence_token_texts}\")\n            \n            \n            generated_ids = self.model.generate(\n                    **model_inputs,\n                    do_sample=False,\n                    temperature=None,\n                    top_p=None,\n                    top_k=None,\n                    max_new_tokens=max_new_tokens\n                )\n            generated_ids = [\n                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n            ]\n            generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n            len_sentence = len(self.tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)[\"input_ids\"])\n            # print()\n            \n        else:\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n            len_sentence = len(inputs[\"input_ids\"])\n            output = self.model.generate(\n                **inputs, \n                do_sample=False,\n                temperature=None,\n                top_p=None,\n                top_k=None,\n                max_new_tokens=1)\n            # print(f\"output: {output}\")\n            generated_ids = [\n                output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output)\n            ]\n            generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n        \n        self.answers.append(generated_text)\n        # answer = \"1\" if \"option1\" in generated_text else \"2\" if \"option2\" in generated_text else None\n        \n        return generated_text, len_sentence\n\n    def batch_inference(self, prompts):\n        \"\"\"\n        Perform batch inference.\n        \"\"\"\n        inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(self.model.device)\n        outputs = self.model.generate(**inputs, max_new_tokens=50)\n        generated_texts = []\n        for input_ids, output_ids in zip(inputs.input_ids, outputs):\n            generated_ids = [\n                output_ids[len(input_ids):]\n            ]\n            generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n            generated_texts.append(generated_text)\n        self.answers.extend(generated_texts)\n        return generated_texts\n\n    def eval(self):\n        \"\"\"\n        Evaluates the predictions against the references \n        \"\"\"\n        accuracy_results = self.acc_metric.compute(predictions=self.predictions, references=self.references)\n\n        precision_results = self.precision_metric.compute(predictions=self.predictions, references=self.references, average=\"macro\")\n\n        recall_results = self.recall_metric.compute(predictions=self.predictions, references=self.references, average=\"macro\")\n\n        results = {\n            'acc' : accuracy_results,\n            'prec' : precision_results,\n            'recall' : recall_results\n        }\n        self.eval_results.append(results)\n        print(\"Evaluation Results:\\n\", results)\n        return results\n\n    \n    def parse_save_output(self, generated_text, unique_labels, current_labeltexts, currentlabel):\n        \"\"\"\n        generated_text: output from model.generate (e.g. '1', 'option 1', 'anna')\n        unique_labels: unique labels from dataset (list of string e.g. [\"1\", \"2\"])\n        current_labeltexts: the current label options (dict of current label text e.g. {1:'anna', 2:'susan'})\n        currentlabel = the current reference label from dataset (string, the dataset['answer']. e.g. \"2\", \"1\" )\n        \"\"\"\n        pred = None\n        clean_generated_text = generated_text.lower().strip()\n        for i in unique_labels:\n            if str(i) in clean_generated_text or current_labeltexts[int(i)].lower().strip() in clean_generated_text or current_labeltexts[int(i)].lower().removeprefix(\"the\").strip() in clean_generated_text :\n                pred = i\n                break\n        # print(f\"gt: {generated_text}\")\n        # print(f\"pred : {pred}\")\n        if pred == None:\n            pred = \"9\"\n        self.predictions.append(pred)\n        self.references.append(currentlabel)\n        ans_dict = ({'pred_answer':pred, 'pred_answertext' : generated_text})\n        # print(f\"ans_dict {ans_dict}\")\n        self.df.loc[len(self.df)] = ans_dict\n        return pred\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:00:54.696621Z","iopub.execute_input":"2025-02-27T00:00:54.697058Z","iopub.status.idle":"2025-02-27T00:01:25.341591Z","shell.execute_reply.started":"2025-02-27T00:00:54.697014Z","shell.execute_reply":"2025-02-27T00:01:25.340607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(\"***REMOVED***\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:01:25.343302Z","iopub.execute_input":"2025-02-27T00:01:25.343960Z","iopub.status.idle":"2025-02-27T00:01:25.503053Z","shell.execute_reply.started":"2025-02-27T00:01:25.343929Z","shell.execute_reply":"2025-02-27T00:01:25.501974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T00:01:25.504314Z","iopub.execute_input":"2025-02-27T00:01:25.504671Z","iopub.status.idle":"2025-02-27T00:01:25.514080Z","shell.execute_reply.started":"2025-02-27T00:01:25.504639Z","shell.execute_reply":"2025-02-27T00:01:25.512911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"massive_gemma = InferenceModel(\"google/gemma-2-2b-it\", device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmassive_qwen = InferenceModel(\"Qwen/Qwen2.5-0.5B-Instruct\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:32:49.794196Z","iopub.execute_input":"2025-02-24T01:32:49.794581Z","iopub.status.idle":"2025-02-24T01:33:26.742960Z","shell.execute_reply.started":"2025-02-24T01:32:49.794543Z","shell.execute_reply":"2025-02-24T01:33:26.741963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xcopa_gemma = InferenceModel(\"google/gemma-2-2b-it\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:33:26.743867Z","iopub.execute_input":"2025-02-24T01:33:26.744174Z","iopub.status.idle":"2025-02-24T01:35:48.718154Z","shell.execute_reply.started":"2025-02-24T01:33:26.744142Z","shell.execute_reply":"2025-02-24T01:35:48.717517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xcopa_qwen = InferenceModel(\"Qwen/Qwen2.5-0.5B-Instruct\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T13:14:37.574264Z","iopub.execute_input":"2025-02-20T13:14:37.574605Z","iopub.status.idle":"2025-02-20T13:14:37.581168Z","shell.execute_reply.started":"2025-02-20T13:14:37.574573Z","shell.execute_reply":"2025-02-20T13:14:37.580314Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The neurons we're interested is in act_fn in Qwen2MLP","metadata":{}},{"cell_type":"markdown","source":"### start playground","metadata":{}},{"cell_type":"code","source":"infer_model = massive_qwen\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T00:39:54.791121Z","iopub.status.idle":"2025-02-13T00:39:54.791494Z","shell.execute_reply":"2025-02-13T00:39:54.791332Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- ada 2 dict untuk setiap language: \n-> *raw_values_avg_tokens* dan *raw_values_last_token*:\n\nkey-nya nama layer dari layer pertama sampe terakhir. setiap valuenya\nukurannya (num_row_in_dataset, hidden_dim) ini neuronnya bisa averaged tokens atau last token. output keluaran suatu neuron adlaah (batch_size, seq_len, num_neurons_di_suatu_layer) tapi seq_len menjadi 1 karena diambil averagenya (raw_values_avg_tokens) atau last token (raw_values_last_token) dari seq) sehingga (num_row_in_dataset, 1, hidden_dim) dijadiin (num_row_in_dataset, hidden_dim) aja.\n\n\n- tensor *full_raw_values_avg_tokens* dan *full_raw_values_last_token*: adalah tensor dari raw_values_avg_tokens atau raw_values_last_token yang nyatuin semua neurons dari semua layer. ukurannya (batch_size, num_neurons_total_seluruh_layer)\n\n\n- tensor *full_raw_values*: adalah tensor yang nyatuin full_raw_values_avg_tokens dan full_raw_values_last_token. ukurannya (2, batch_size, num_neurons_total_seluruh_layer). yg pertama full_raw_values_avg_tokens kedua full_raw_values_last_token.\n","metadata":{}},{"cell_type":"code","source":"full_neurons_r = torch.randn(1, 1, 1, 3)\nprint(full_neurons)\nactivation_dict = {}\n\nfull_neurons = full_neurons_r.transpose(-3,-4)[0].transpose(-1,-2)\nss = (full_neurons.mean(dim=-1).topk(2).indices)\nfor i in range (full_neurons_r.size(0)):\n    activation_dict[i] = ss[i]\n\nactivation_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:39:45.694508Z","iopub.execute_input":"2025-02-21T04:39:45.694823Z","iopub.status.idle":"2025-02-21T04:39:45.704299Z","shell.execute_reply.started":"2025-02-21T04:39:45.694799Z","shell.execute_reply":"2025-02-21T04:39:45.703236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# IMP: ini buat nyatuin tiap entry di dataset\nimport torch\n\ntensor1 = torch.randn(1, 2, 3)  # Shape: (1, 2, 3)\nprint(tensor1)\ntensor2 = torch.randn(2, 2, 3)  # Shape: (2, 2, 3)\nprint(tensor2)\n# Concatenate along dim=0\nresult = torch.cat([tensor1, tensor2], dim=0)\n\nprint(result.shape)  # Output: (3, 2, 3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T11:45:18.649480Z","iopub.execute_input":"2025-02-05T11:45:18.649872Z","iopub.status.idle":"2025-02-05T11:45:18.658463Z","shell.execute_reply.started":"2025-02-05T11:45:18.649843Z","shell.execute_reply":"2025-02-05T11:45:18.657647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tensor2 = torch.tensor([[1, 2], [3, 4]])  # Shape: [1, 2]\n# print(tensor2.float().mean(dim=1))\nprint(tensor2.float()[-1, :])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:46:19.797093Z","iopub.execute_input":"2025-02-05T17:46:19.797543Z","iopub.status.idle":"2025-02-05T17:46:19.803685Z","shell.execute_reply.started":"2025-02-05T17:46:19.797499Z","shell.execute_reply":"2025-02-05T17:46:19.802969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tensor2 = torch.tensor([[1, 2], [3, 4]])  # Shape: [1, 2]\nprint(tensor2.shape)\nlists = [tensor2, tensor2, tensor2]\nstacked = torch.stack(lists, dim=0)\nprint(stacked.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T18:37:43.967008Z","iopub.execute_input":"2025-02-05T18:37:43.967332Z","iopub.status.idle":"2025-02-05T18:37:43.973511Z","shell.execute_reply.started":"2025-02-05T18:37:43.967302Z","shell.execute_reply":"2025-02-05T18:37:43.972680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw_values_avg_tokens = {}\nraw_values_last_token = {}\n# ini yg raw value\n\n\n# def aggregate_across_tokens(output, is_averaged_tokens):\n#     if is_averaged_tokens:\n#         output = output.float().mean(dim=1, keepdim=True) \n#     else:\n#         output = output.float()[:, -1:] # get the last token\n#     return output\n\ndef save_raw_vals_to_dict(name, dictname, savees):\n    if not name in dictname:\n        dictname[name] = savees\n    else:\n        dictname[name] = torch.cat([dictname[name], savees], dim=0)\n\ndef coba(name, is_averaged_tokens, output):\n    # output is always (batch size, 1, hidden dim)\n    # print(f\"Layer: {name}, output Shape: {output.shape}\")\n    # print(\"Output\", output)\n    # output is always (batch size, hidden dim), where hidden dim is the number of neurons in a layer.\n    output \n    if is_averaged_tokens: # simpen average value across tokens\n        output = output.float().mean(dim=1) \n        save_raw_vals_to_dict(name, raw_values_avg_tokens, output)\n    else:\n        output = output.float()[-1,:] # simpen the last token\n        save_raw_vals_to_dict(name, raw_values_last_token, output)\n        \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T17:47:07.370504Z","iopub.execute_input":"2025-02-05T17:47:07.370804Z","iopub.status.idle":"2025-02-05T17:47:07.376049Z","shell.execute_reply.started":"2025-02-05T17:47:07.370783Z","shell.execute_reply":"2025-02-05T17:47:07.375070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = torch.tensor([[[1, 2], [3, 4]]])\ny = torch.tensor([[[5, 6], [7, 8]]])\nz = torch.tensor([[[1, 6], [3, 2]]])\n\n# layer_outputs['a'] = x\ncoba(\"a\", True, x)\ncoba(\"a\", True, y)\nprint(raw_values_avg_tokens['a'])\nprint(raw_values_avg_tokens['a'].shape)\n\ncoba(\"a\", False, y)\ncoba(\"a\", False, z)\nprint(raw_values_last_token['a'])\nprint(raw_values_last_token['a'].shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T17:36:06.371843Z","iopub.execute_input":"2025-02-20T17:36:06.372224Z","iopub.status.idle":"2025-02-20T17:36:06.396931Z","shell.execute_reply.started":"2025-02-20T17:36:06.372197Z","shell.execute_reply":"2025-02-20T17:36:06.395895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = torch.tensor([[[1, 2], [3, 4]]])\n# layer_outputs['a'] = x\ncoba(\"b\", True, y)\ncoba(\"b\", True, y)\nprint(raw_values_avg_tokens['b'])\nprint(raw_values_avg_tokens['b'].shape)\n\ncoba(\"b\", False, x)\ncoba(\"b\", False, x)\n\nprint(raw_values_last_token['b'])\nprint(raw_values_last_token['b'].shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T16:40:08.059017Z","iopub.execute_input":"2025-02-05T16:40:08.059349Z","iopub.status.idle":"2025-02-05T16:40:08.067950Z","shell.execute_reply.started":"2025-02-05T16:40:08.059323Z","shell.execute_reply":"2025-02-05T16:40:08.067141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list(raw_values_avg_tokens.values())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T16:50:48.002737Z","iopub.execute_input":"2025-02-05T16:50:48.003069Z","iopub.status.idle":"2025-02-05T16:50:48.009485Z","shell.execute_reply.started":"2025-02-05T16:50:48.003038Z","shell.execute_reply":"2025-02-05T16:50:48.008719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# IMP: nyatuin tiap layer OKE\nfull_raw_values_avg_tokens = list(raw_values_avg_tokens.values())[0]\nfull_raw_values_last_token = list(raw_values_last_token.values())[0]\n\n# print(full_raw_values_avg_tokens.shape)\n# print(full_raw_values_last_token.shape)\nfor i in list(raw_values_avg_tokens.values())[1:]:\n    # print(i.shape)\n    full_raw_values_avg_tokens = torch.cat([full_raw_values_avg_tokens, i], dim=-1)\nfor i in list(raw_values_last_token.values())[1:]:\n    full_raw_values_last_token = torch.cat([full_raw_values_last_token, i], dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T16:54:44.412303Z","iopub.execute_input":"2025-02-05T16:54:44.412640Z","iopub.status.idle":"2025-02-05T16:54:44.419104Z","shell.execute_reply.started":"2025-02-05T16:54:44.412616Z","shell.execute_reply":"2025-02-05T16:54:44.418249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(full_raw_values_avg_tokens)\nprint(full_raw_values_last_token)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T16:55:15.210920Z","iopub.execute_input":"2025-02-05T16:55:15.211250Z","iopub.status.idle":"2025-02-05T16:55:15.217667Z","shell.execute_reply.started":"2025-02-05T16:55:15.211222Z","shell.execute_reply":"2025-02-05T16:55:15.216829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_raw_values_last_token.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T16:59:46.860896Z","iopub.execute_input":"2025-02-05T16:59:46.861204Z","iopub.status.idle":"2025-02-05T16:59:46.866125Z","shell.execute_reply.started":"2025-02-05T16:59:46.861180Z","shell.execute_reply":"2025-02-05T16:59:46.865184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# IMP: nyatuin avg dan last\nfull_raw_values = torch.stack((full_raw_values_avg_tokens, full_raw_values_last_token), dim=0)\n# stack buat nyatuin avg token dan last token OKE\n# torch.stack((raw_values_last_token['a'], raw_values_avg_tokens['a']), dim=0)\nfull_raw_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T16:59:04.949228Z","iopub.execute_input":"2025-02-05T16:59:04.949607Z","iopub.status.idle":"2025-02-05T16:59:04.956308Z","shell.execute_reply.started":"2025-02-05T16:59:04.949575Z","shell.execute_reply":"2025-02-05T16:59:04.955596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"full_raw_values.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-05T16:59:18.315648Z","iopub.execute_input":"2025-02-05T16:59:18.315959Z","iopub.status.idle":"2025-02-05T16:59:18.321114Z","shell.execute_reply.started":"2025-02-05T16:59:18.315936Z","shell.execute_reply":"2025-02-05T16:59:18.320270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ex = torch.tensor(\n    [[[[0, 1, 2, 3],\n        [4, 5, 6, 7]]]]\n)\nex.transpose(-1,-2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:57:16.415638Z","iopub.execute_input":"2025-02-20T16:57:16.415978Z","iopub.status.idle":"2025-02-20T16:57:16.422861Z","shell.execute_reply.started":"2025-02-20T16:57:16.415955Z","shell.execute_reply":"2025-02-20T16:57:16.421982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### === end playground","metadata":{}},{"cell_type":"markdown","source":"### prompt dengan predict intent","metadata":{}},{"cell_type":"code","source":"\ndef check_index_prompt(when, prompt, model_name): # TODO: method ini pindahin ke class infer_model dan hapus printnya \n    \"\"\"\n    return len tokenized text when it is cut (excluding the chat template), len tokenized text \n    \"\"\"\n    \n    if model_name == 'Qwen/Qwen2.5-0.5B-Instruct':\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": prompt}\n            ]\n        text = infer_model.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        sentence_tokens = infer_model.tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n        sentence_ids = sentence_tokens[\"input_ids\"]\n        sentence_token_texts = infer_model.tokenizer.convert_ids_to_tokens(sentence_ids)[:-5]\n        # print(f\"{when} sentence_token_texts {sentence_token_texts}. len(sentence_token_texts) {len(sentence_token_texts)}\")\n        return len(sentence_token_texts), len(sentence_ids)\n    elif model_name == 'google/gemma-2-2b-it':\n        messages = [\n            {\"role\": \"user\", \"content\": prompt}\n            ]\n        text = infer_model.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n        sentence_tokens = infer_model.tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)\n        sentence_ids = sentence_tokens[\"input_ids\"]\n        sentence_token_texts = infer_model.tokenizer.convert_ids_to_tokens(sentence_ids)[:-5]\n        # print(f\"{when} sentence_token_texts {sentence_token_texts}. len(sentence_token_texts) {len(sentence_token_texts)}\")\n        return len(sentence_token_texts), len(sentence_ids)\n\n\ndef get_index_start_end_prompt(data, model_name, dataset_name): # TODO: method ini pindahin ke class infer_model dan hapus printnya \n    \"\"\"\n    return id start, id end, len sentence tokenize, prompt whole\n    buat massive\n    \"\"\"\n    prompt_before = prompt_after = prompt_whole = \"\"\n    if dataset_name == \"AmazonScience/massive\":\n        utterance, options = data\n        prompt_before = (\n            f\"\"\"\n            Instruction: Classify the intent of the following utterance.\n            Utterance: \n            \"\"\"\n            )\n        prompt_after = (\n            f\"\"\"\n            Instruction: Classify the intent of the following utterance.  \n            Utterance: {utterance}.  \n            \"\"\"\n            )\n        prompt_whole = (\n            f\"\"\"\n            Instruction: Classify the intent of the following utterance.  \n            Utterance: {utterance}.  \n            Options: {options}. \n            Intent: \n            \"\"\"\n            )\n    elif dataset_name == \"xcopa\":\n        premise, choice1, choice2, question_type = data\n        prompt_before = (\n            f\"\"\"\n            Premise: \n            \"\"\")\n        prompt_after = (\n            f\"\"\"\n            Premise: {premise}\n            \"\"\")\n        prompt_whole = (\n            f\"\"\"\n            Premise: {premise}\n            I'm hesitating between the two options. Help me choose the {question_type}: \n            - {choice1} \n            - {choice2}\n            \"\"\")\n    if prompt_before == \"\":\n        print(\"dataset invalid\")\n    id_prompt_start, _ = check_index_prompt(\"prompt_before\", prompt_before, model_name)\n    id_prompt_end, _ = check_index_prompt(\"prompt_after\", prompt_after, model_name)\n    _, len_sentence = check_index_prompt(\"prompt_whole\", prompt_whole, model_name)\n    return id_prompt_start, id_prompt_end, len_sentence, prompt_whole\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T05:54:53.697654Z","iopub.execute_input":"2025-02-20T05:54:53.698011Z","iopub.status.idle":"2025-02-20T05:54:53.705531Z","shell.execute_reply.started":"2025-02-20T05:54:53.697989Z","shell.execute_reply":"2025-02-20T05:54:53.704566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import psutil\n\ndef check_cpu_memory():\n    mem = psutil.virtual_memory()\n    print(f\"Total Memory: {mem.total / 1024**3:.2f} GB\")\n    print(f\"Available Memory: {mem.available / 1024**3:.2f} GB\")\n    print(f\"Used Memory: {mem.used / 1024**3:.2f} GB\")\n    print(f\"Memory Usage: {mem.percent}%\")\ncheck_cpu_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T00:02:04.772060Z","iopub.execute_input":"2025-02-20T00:02:04.772430Z","iopub.status.idle":"2025-02-20T00:02:04.779427Z","shell.execute_reply.started":"2025-02-20T00:02:04.772398Z","shell.execute_reply":"2025-02-20T00:02:04.778566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T00:18:57.800353Z","iopub.execute_input":"2025-02-20T00:18:57.800644Z","iopub.status.idle":"2025-02-20T00:18:58.021978Z","shell.execute_reply.started":"2025-02-20T00:18:57.800622Z","shell.execute_reply":"2025-02-20T00:18:58.021007Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Per Model Visualization","metadata":{}},{"cell_type":"code","source":"num_layer_gemma2 = 26 \nnum_layer_qwen05 = 24","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:19:57.406659Z","iopub.execute_input":"2025-02-27T14:19:57.411018Z","iopub.status.idle":"2025-02-27T14:19:57.430076Z","shell.execute_reply.started":"2025-02-27T14:19:57.410895Z","shell.execute_reply":"2025-02-27T14:19:57.427933Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualize distribution","metadata":{}},{"cell_type":"code","source":"import torch \nfrom datasets import get_dataset_config_names\nfrom tqdm import tqdm\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport scipy.stats as stats\n\n\ndef hist_neuron_values(num_neurons, num_layer, data, label=\"\"):\n    \"\"\"\n    data isinya untuk seluruh neuron di 1 data point 1 bahasa. co: full_languages_raw_values[0][0][0]\n    \"\"\"\n    \n    num_points = num_neurons  # 116736\n    step_size = num_neurons / num_layer\n    # step_size = 24\n    x_indices = np.arange(0, num_points, step_size)  \n    x_labels = [str(i) for i in x_indices]  \n    \n    # Plot the data\n    plt.figure(figsize=(7, 2))\n    plt.plot(data.numpy(), label=\"Tensor Values\", linewidth=0.8)  \n    plt.xticks(x_indices, x_labels, rotation=45) \n    plt.xlabel(\"Index\")\n    plt.ylabel(\"Value\")\n    plt.title(f\"Tensor Value Distribution {label}\")\n    plt.legend()\n    plt.show()\n\n\n\ndef kstest(tensor):\n    \"\"\"\n    tensornya isi untuk seluruh neuron di 1 data point 1 bahasa. co: full_languages_raw_values[0][0][0]\n    \"\"\"\n    # List of distributions to test\n    distributions = [\"norm\", \"expon\", \"gamma\", \"lognorm\", \"beta\", \"weibull_min\"]\n    \n    best_fit = None\n    best_pvalue = 0\n    \n    for dist_name in distributions:\n        dist = getattr(stats, dist_name)\n        params = dist.fit(tensor)  # Fit distribution to data\n        stat, p_value = stats.kstest(tensor, dist_name, args=params)  # KS test\n    \n        print(f\"{dist_name}: p-value = {p_value:.4f}\")\n    \n        if p_value > best_pvalue:\n            best_pvalue = p_value\n            best_fit = dist_name\n    \n    print(f\"\\nBest fitting distribution: {best_fit}\")\n# kstest(full_languages_raw_values[0][0][0])\n\n\ndef plot_kde(tensor):\n    \"\"\"\n    tensornya isi untuk seluruh neuron di 1 data point 1 bahasa. co:\n    full_languages_raw_values[0][0][0]\n    \"\"\"\n    sns.kdeplot(tensor, bw_adjust=0.5)\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Density\")\n    plt.title(\"Tensor Value Distribution (KDE)\")\n    plt.show()\n\ndef get_language_dict(dataset_name):\n    dataset_name = dataset_name\n    configs = get_dataset_config_names(dataset_name, trust_remote_code=True)\n    n_instances = 0\n    n_lang = 0\n    language_dict = {}\n    # register_hook(infer_model, handlers)\n    \n    for lang in enumerate([lang for lang in configs if not lang.startswith(\"all\") and not lang.startswith(\"translation\")]):\n        # if n_lang >= 2:\n        language_dict[lang[0]] = lang[1]\n        # print(lang)\n    \n    return language_dict\n\n\ndef clear_cuda_cache():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n\n\ndef show_stats(tensor):\n    \"\"\"\n    tensor:full_languages_raw_values[lang1][0][0] / 1 language semua neuron dan rows\n    \"\"\"\n    print(\"Mean:\", torch.mean(tensor).item())\n    print(\"Median:\", torch.median(tensor).item())\n    print(\"Standard Deviation:\", torch.std(tensor).item())\n    print(\"Min:\", torch.min(tensor).item())\n    print(\"Max:\", torch.max(tensor).item())\n    \n    # Mode Calculation\n    mode_value, mode_count = torch.mode(tensor)\n    print(\"Mode:\", mode_value.item())\n\ndef visualize_stats(tensor, a_lang, a_row=0):\n    \"\"\"\n    tensor: full raw values dim 4\n    a_lang: index of language to visualize\n    \"\"\"\n    tensor_lang_row = tensor[a_lang][0][a_row]\n    show_stats(tensor_lang_row)\n    plot_kde(tensor_lang_row)\n    kstest(tensor_lang_row)\n    \ndef compare_hist(tensor, num_layer, lang1, row1, lang2, row2, lang_dict, top=10):\n    \"\"\"\n    tensor: full raw values dim 4\n    a_lang: index of language to visualize\n    \"\"\"\n\n\n    \n    tensor1a = tensor[lang1][0][row1]\n    tensor1b = tensor[lang1][0][row2]\n\n    tensor2a = tensor[lang2][0][row1]\n    tensor2b = tensor[lang2][0][row2]\n    \n    hist_neuron_values(tensor.size(-1), num_layer, tensor1a, f\"{row1} in {lang_dict[lang1]}\")\n    print(f\"topk {row1} in {lang_dict[lang1]}: {tensor1a.topk(top)}\")\n    hist_neuron_values(tensor.size(-1), num_layer, tensor1b, f\"{row2} in {lang_dict[lang1]}\")\n    print(f\"topk {row2} in {lang_dict[lang1]}: {tensor1b.topk(top)}\")\n\n    hist_neuron_values(tensor.size(-1), num_layer, tensor2a, f\"{row1} in {lang_dict[lang2]}\")\n    print(f\"topk {row1} in {lang_dict[lang2]}: {tensor2a.topk(top)}\")\n\n    hist_neuron_values(tensor.size(-1), num_layer, tensor2b, f\"{row2} in {lang_dict[lang2]}\")\n    print(f\"topk {row2} in {lang_dict[lang2]}: {tensor2b.topk(top)}\")\n\nmassive_lang_dict = get_language_dict(\"AmazonScience/massive\")\nxcopa_lang_dict = get_language_dict(\"xcopa\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:19:58.354181Z","iopub.execute_input":"2025-02-27T14:19:58.354810Z","iopub.status.idle":"2025-02-27T14:20:00.835069Z","shell.execute_reply.started":"2025-02-27T14:19:58.354752Z","shell.execute_reply":"2025-02-27T14:20:00.833279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸Massive Qwen","metadata":{}},{"cell_type":"code","source":"full_languages_raw_values = torch.load(\"/kaggle/input/raw-qwen05-predict-200/raw_qwen05_predict_massive200_selected_v.pt\", weights_only=False)\nprint(full_languages_raw_values.shape)\na_lang = 0\nprint(f\"lang 0\")\nvisualize_stats(full_languages_raw_values, a_lang, a_row=0)\ncompare_hist(full_languages_raw_values, num_layer=24, lang1=0, row1=0, lang2=19, row2=199, lang_dict=massive_lang_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:20:42.025255Z","iopub.execute_input":"2025-02-27T14:20:42.026823Z","iopub.status.idle":"2025-02-27T14:21:21.646758Z","shell.execute_reply.started":"2025-02-27T14:20:42.026751Z","shell.execute_reply":"2025-02-27T14:21:21.645380Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸XCOPA Gemma","metadata":{}},{"cell_type":"code","source":"full_languages_raw_values_xcopa_gemma = torch.load(\"/kaggle/input/raw-gemma2-predictv1/raw_gemma2_predict_xcopa200.pt\", weights_only=False)\n# full_languages_raw_values_xcopa_gemma.shape\n# print(full_languages_raw_values_xcopa_gemma.shape)\na_lang = 0\nprint(f\"lang 0\")\n\nvisualize_stats(full_languages_raw_values_xcopa_gemma, a_lang, a_row=0)\ncompare_hist(full_languages_raw_values_xcopa_gemma, num_layer=26, lang1=0, row1=0, lang2=10, row2=19, lang_dict=xcopa_lang_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:21:21.648636Z","iopub.execute_input":"2025-02-27T14:21:21.649142Z","iopub.status.idle":"2025-02-27T14:22:10.474460Z","shell.execute_reply.started":"2025-02-27T14:21:21.649098Z","shell.execute_reply":"2025-02-27T14:22:10.473201Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸XCOPA Qwen","metadata":{}},{"cell_type":"code","source":"full_languages_raw_values_xcopa_qwen = torch.load(\"/kaggle/input/raw-qwen05-predict-200/raw_qwen05_predict_xcopa200.pt\", weights_only=False)\nprint(full_languages_raw_values_xcopa_qwen.shape)\na_lang = 0\nprint(f\"lang 0\")\n\nvisualize_stats(full_languages_raw_values_xcopa_qwen, a_lang, a_row=0)\ncompare_hist(full_languages_raw_values_xcopa_qwen, num_layer=num_layer_qwen05, lang1=0, row1=0, lang2=10, row2=19, lang_dict=xcopa_lang_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:22:10.476660Z","iopub.execute_input":"2025-02-27T14:22:10.477075Z","iopub.status.idle":"2025-02-27T14:22:38.440118Z","shell.execute_reply.started":"2025-02-27T14:22:10.477040Z","shell.execute_reply":"2025-02-27T14:22:38.438813Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸Massive Gemma","metadata":{}},{"cell_type":"code","source":"full_languages_raw_values_massive_gemma = torch.load(\"/kaggle/input/raw-gemma2-predictv1/raw_gemma2_predict_massive200.pt\", weights_only=False)\nprint(full_languages_raw_values_massive_gemma.shape)\na_lang = 0\nprint(f\"lang 0\")\nvisualize_stats(full_languages_raw_values_massive_gemma, a_lang, a_row=0)\ncompare_hist(full_languages_raw_values_massive_gemma, num_layer=num_layer_gemma2, lang1=0, row1=0, lang2=19, row2=199, lang_dict=massive_lang_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:22:38.441905Z","iopub.execute_input":"2025-02-27T14:22:38.442223Z","iopub.status.idle":"2025-02-27T14:24:16.559741Z","shell.execute_reply.started":"2025-02-27T14:22:38.442195Z","shell.execute_reply":"2025-02-27T14:24:16.558448Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pairing antar language","metadata":{}},{"cell_type":"markdown","source":"#### Kesamaan index neuron yang teraktivasi.\n- Case 1: Neuron yang teraktivasi -> yang actv. valuenya di atas 0 untuk seluruh data point di dataset\n- Case 2: Neuron yang teraktivasi -> anggap abs(actv_value) > 0.5 = 1 -> neuron yang probability munculnya > meannya itu yang teraktivasi","metadata":{}},{"cell_type":"markdown","source":"#### ===== activation value dengan prompt tanpa predict (fully in their own languages)","metadata":{}},{"cell_type":"markdown","source":"case 1","metadata":{}},{"cell_type":"markdown","source":"di sini anggap activated neuron (avg token) yang di setiap row di dataset(cuman 2 row) actv valuenya > 0","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom scipy.stats import entropy\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\ndef get_k_lang_actv_dict(k, full_neurons, method=\"default\", topk=0):\n    \n    \"\"\"\n    di sini anggap activated neuron (avg token) yang di setiap row di dataset(cuman 2 row) actv valuenya > 0.\n    k = num language\"\"\"\n    activation_dict = {}\n    \n    full_neurons = full_neurons.transpose(-3,-4)[0].transpose(-1,-2)\n\n    if method == \"default\":\n        for i in range (full_neurons.size(0)):\n            tensor_lang = full_neurons[i]\n            rows_with_both_positive = (tensor_lang > 0).all(dim=-1)\n            \n            indices = torch.where(rows_with_both_positive)[0]\n            activation_dict[i] = indices\n            # indices.shape\n            # print(indices.shape)\n    elif method == \"topk\":\n        if topk==0:\n            print(f\"topk must not be 0\")\n        top = (full_neurons.mean(dim=-1).topk(topk).indices)\n        for i in range (full_neurons.size(0)):\n            activation_dict[i] = top[i]\n    \n    return activation_dict\n\n# Step 1: Convert lists to probability distributions\ndef to_probability_distribution(values):\n    total = sum(values)\n    return [v / total for v in values]\n\n\ndef make_heatmap_neuron_overlap(activation_dict, k, with_label=True, method=\"default\", alpha=1, normalized =False):    \n    # Example dictionary: keys 0-52, values are 1D tensors of activated neuron indices\n    # activation_dict = get_k_lang_actv_dict(10)\n\n    overlap_matrix = torch.tensor([])\n    if method == \"default\":\n        # Step 1: Create a binary matrix\n        max_neuron_index = max(max(indices) for indices in activation_dict.values()) + 1  # Find the maximum neuron index\n        binary_matrix = torch.zeros((k+1, max_neuron_index), dtype=torch.int)  # Initialize binary matrix\n        \n        for key, indices in activation_dict.items():\n            binary_matrix[key, indices] = 1  # Set activated neurons to 1\n        \n        # Step 2: Compute overlaps (dot product between rows)\n        overlap_matrix = torch.matmul(binary_matrix, binary_matrix.T)  # Dot product of binary_matrix with its transpose\n\n    elif method == \"jaccard\":\n        max_neuron_index = max(max(indices) for indices in activation_dict.values()) + 1\n        binary_matrix = torch.zeros((k+1, max_neuron_index), dtype=torch.int)\n    \n        # Fill binary matrix with activation data\n        for key, indices in activation_dict.items():\n            binary_matrix[key, indices] = 1  \n    \n        # Compute Jaccard distance matrix\n        overlap_matrix = torch.zeros((k+1, k+1))\n    \n        for i in range(k+1):\n            for j in range(k+1):\n                intersection = (binary_matrix[i] & binary_matrix[j]).sum().item()\n                union = (binary_matrix[i] | binary_matrix[j]).sum().item()\n                jaccard_similarity = intersection / union if union > 0 else 0\n                overlap_matrix[i, j] = jaccard_similarity\n        overlap_matrix = overlap_matrix ** alpha\n        if normalized:\n            overlap_matrix = overlap_matrix / overlap_matrix.sum(axis=1, keepdims=True)\n\n    \n    # Step 3: Visualize the heatmap\n    plt.figure(figsize=(10, 8))\n    if with_label:\n        sns.heatmap(overlap_matrix.numpy(), annot=True, fmt=\"d\", cmap=\"YlOrRd\", \n                    xticklabels=range(k+1), yticklabels=range(k+1))\n    else:\n        sns.heatmap(overlap_matrix.numpy(), fmt=\"d\", cmap=\"YlOrRd\",cbar=True)\n    plt.xlabel(\"Key\")\n    plt.ylabel(\"Key\")\n    plt.title(f\"Overlap Heatmap of Activated Neurons: {method}\")\n    plt.show()\n    return overlap_matrix\n\n\ndef normed_heatmap_neuron_overlap(num_lang, activation_dict):\n    # Step 1: Create a binary matrix\n    max_neuron_index = max(max(indices) for indices in activation_dict.values()) + 1  # Find the maximum neuron index\n    binary_matrix = torch.zeros((num_lang, max_neuron_index), dtype=torch.int)  # Initialize binary matrix\n    \n    for key, indices in activation_dict.items():\n        binary_matrix[key, indices] = 1  # Set activated neurons to 1\n    \n    # Step 2: Compute overlaps (dot product between rows)\n    overlap_matrix = torch.matmul(binary_matrix, binary_matrix.T)  # Dot product of binary_matrix with its transpose\n    \n    # Step 3: Normalize the overlap matrix\n    # Compute the number of activated neurons for each key\n    num_activated_neurons = binary_matrix.sum(dim=1, keepdim=True)\n    \n    # Normalize by the minimum number of activated neurons between each pair of keys\n    normalized_overlap_matrix = overlap_matrix / torch.minimum(\n        num_activated_neurons, num_activated_neurons.T\n    )\n    \n    # Ensure the diagonal is exactly 1 (self-overlap is always 1)\n    normalized_overlap_matrix.fill_diagonal_(1)\n    \n    # Step 4: Visualize the normalized heatmap (no labels or annotations)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(normalized_overlap_matrix.numpy(), cmap=\"YlOrRd\", cbar=True, vmin=0, vmax=1)\n    plt.xlabel(\"Key\")\n    plt.ylabel(\"Key\")\n    plt.title(\"Normalized Overlap Heatmap of Activated Neurons\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:24:16.561075Z","iopub.execute_input":"2025-02-27T14:24:16.561530Z","iopub.status.idle":"2025-02-27T14:24:16.580694Z","shell.execute_reply.started":"2025-02-27T14:24:16.561475Z","shell.execute_reply":"2025-02-27T14:24:16.579353Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### =====activation value dengan prompt predict (prompts in english)","metadata":{}},{"cell_type":"markdown","source":"#### Visualisasi distribusi neurons ","metadata":{}},{"cell_type":"markdown","source":"lihat distribusi nilai neurons ","metadata":{}},{"cell_type":"markdown","source":"very similar neurons activated for prompts in english for every language\n\n-> may need to sort out the specific only to the pair of language","metadata":{}},{"cell_type":"markdown","source":"case 1","metadata":{}},{"cell_type":"code","source":"def visualize_overlap(num_lang , tensor, method=\"default\", topk=0):\n    \"\"\"\n    tensor: full neurons 4 dim\n    \"\"\"\n    activation_dict = get_k_lang_actv_dict(num_lang, tensor, method, topk)\n    make_heatmap_neuron_overlap(activation_dict, num_lang, False)\n    normed_heatmap_neuron_overlap(num_lang, activation_dict)\n    make_heatmap_neuron_overlap(activation_dict, num_lang, False, \"jaccard\", 2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:24:16.582391Z","iopub.execute_input":"2025-02-27T14:24:16.582940Z","iopub.status.idle":"2025-02-27T14:24:16.608955Z","shell.execute_reply.started":"2025-02-27T14:24:16.582887Z","shell.execute_reply":"2025-02-27T14:24:16.607612Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸Massive Qwen","metadata":{}},{"cell_type":"code","source":"num_lang, _, num_rows, num_neurons = full_languages_raw_values.shape\nvisualize_overlap(num_lang, full_languages_raw_values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:24:16.610341Z","iopub.execute_input":"2025-02-27T14:24:16.610793Z","iopub.status.idle":"2025-02-27T14:24:29.430639Z","shell.execute_reply.started":"2025-02-27T14:24:16.610743Z","shell.execute_reply":"2025-02-27T14:24:29.429412Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸XCOPA Gemma","metadata":{}},{"cell_type":"code","source":"num_lang, _, num_rows, num_neurons = full_languages_raw_values_xcopa_gemma.shape\nvisualize_overlap(num_lang, full_languages_raw_values_xcopa_gemma)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:24:29.433680Z","iopub.execute_input":"2025-02-27T14:24:29.433998Z","iopub.status.idle":"2025-02-27T14:24:33.552807Z","shell.execute_reply.started":"2025-02-27T14:24:29.433972Z","shell.execute_reply":"2025-02-27T14:24:33.551445Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸XCOPA Qwen","metadata":{}},{"cell_type":"code","source":"num_lang, _, num_rows, num_neurons = full_languages_raw_values_xcopa_qwen.shape\nvisualize_overlap(num_lang, full_languages_raw_values_xcopa_qwen)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:24:33.554759Z","iopub.execute_input":"2025-02-27T14:24:33.555160Z","iopub.status.idle":"2025-02-27T14:24:36.553132Z","shell.execute_reply.started":"2025-02-27T14:24:33.555124Z","shell.execute_reply":"2025-02-27T14:24:36.552039Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸Massive Gemma","metadata":{}},{"cell_type":"code","source":"num_lang, _, num_rows, num_neurons = full_languages_raw_values_massive_gemma.shape\nvisualize_overlap(num_lang, full_languages_raw_values_massive_gemma)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:24:36.554395Z","iopub.execute_input":"2025-02-27T14:24:36.554872Z","iopub.status.idle":"2025-02-27T14:24:55.904712Z","shell.execute_reply.started":"2025-02-27T14:24:36.554825Z","shell.execute_reply":"2025-02-27T14:24:55.903460Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Overlap on same model different dataset ","metadata":{}},{"cell_type":"code","source":"# First dictionary with full language names and codes\nlanguages = massive_lang_dict\n\n# Second dictionary with language codes\nsecond_dict = xcopa_lang_dict\n\n# Extract language codes from `languages`\nlang_code_dict = {k: v.split('(')[-1].split('-')[0] for k, v in languages.items()}\n\n# Find matching languages\nmatching_languages = {key: value for key, value in second_dict.items() if value in lang_code_dict.values()}\n\n# Get indices of matching languages in first dictionary\nid_match_massive = [key for key, value in lang_code_dict.items() if value in matching_languages.values()]\n\n# Get indices of matching languages in second dictionary\nid_match_xcopa = list(matching_languages.keys())\n\n# Print results\nprint(\"Indices in first dictionary:\", id_match_massive)\nprint(\"Indices in second dictionary:\", id_match_xcopa)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:24:55.905698Z","iopub.execute_input":"2025-02-27T14:24:55.906138Z","iopub.status.idle":"2025-02-27T14:24:55.915620Z","shell.execute_reply.started":"2025-02-27T14:24:55.906098Z","shell.execute_reply":"2025-02-27T14:24:55.914072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"id_match_massive = id_match_massive[:-1]\nid_match_massive","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:37:06.165397Z","iopub.execute_input":"2025-02-27T14:37:06.165819Z","iopub.status.idle":"2025-02-27T14:37:06.173580Z","shell.execute_reply.started":"2025-02-27T14:37:06.165788Z","shell.execute_reply":"2025-02-27T14:37:06.172009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\n\n# Example data (replace with your actual data)\ndata = torch.randint(0, 116736, (1000,)).tolist()  # Random data between 0 and 116736\n\n# Define bins with step size 4864\nbin_width = 116736 / 24\nbins = torch.arange(0, 116736 + bin_width, bin_width).tolist()  # Create bin edges\n\n# Plot histogram\nplt.hist(data, bins=bins, edgecolor='black', alpha=0.7)\nplt.xlabel(\"Range (bin size = 4864)\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram with Bin Size 4864\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max([2,3,4])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:57:18.181383Z","iopub.execute_input":"2025-02-27T14:57:18.181869Z","iopub.status.idle":"2025-02-27T14:57:18.189562Z","shell.execute_reply.started":"2025-02-27T14:57:18.181833Z","shell.execute_reply":"2025-02-27T14:57:18.188434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\n\ndef overlap_model_diff_dataset(massive_dict, xcopa_dict, num_neurons, num_layer):\n    \"\"\"\n    key in massive_dict is always already in xcopa_dict\n    \"\"\"\n    overlap_dict = {key: list(set(massive_dict[key].tolist()) & set(xcopa_dict[key].tolist())) for key in massive_dict.keys() if key in xcopa_dict}\n    overlap_proportion_dict = dict()\n    for key, value in overlap_dict.items():\n        max_overlap = max([len(massive_dict[key]), len(xcopa_dict[key])])\n        lang = massive_lang_dict[id_match_massive[key]]\n        overlap_proportion_dict[lang] = len(value)/max_overlap\n        label = f\"overlap {lang} massive-xcopa\"\n        bin_width = num_neurons / num_layer\n        bins = torch.arange(0, num_neurons + bin_width, bin_width).tolist()  # Create bin edges\n        plt.figure(figsize=(6, 4))  \n        # Plot histogram\n        plt.hist(value, bins=bins, edgecolor='black', alpha=0.7)\n        plt.xlabel(\"Index\")\n        plt.ylabel(\"Frequency\")\n        plt.title(f\"{label}\")\n        plt.show()\n    print(overlap_proportion_dict)\n    return overlap_dict\n        \n    # print(overlap_proportion_dict)\n        # hist_neuron_values(num_neurons, num_layer, torch.tensor(value), label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T18:06:10.059401Z","iopub.execute_input":"2025-02-27T18:06:10.059839Z","iopub.status.idle":"2025-02-27T18:06:10.069993Z","shell.execute_reply.started":"2025-02-27T18:06:10.059806Z","shell.execute_reply":"2025-02-27T18:06:10.068485Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 🐸Qwen 0.5 on Massive and XCOPA","metadata":{}},{"cell_type":"code","source":"num_lang, _, _, num_neurons = full_languages_raw_values.shape\nactivation_dict = get_k_lang_actv_dict(52, full_languages_raw_values )\nmassive_qwen_dict = {i: activation_dict[k] for i, k in enumerate(id_match_massive) if k in activation_dict}\nactivation_dict = get_k_lang_actv_dict(11, full_languages_raw_values_xcopa_qwen)\nxcopa_qwen_dict = {i: activation_dict[k] for i, k in enumerate(id_match_xcopa) if k in activation_dict}\noverlap_dict = overlap_model_diff_dataset(massive_qwen_dict, xcopa_qwen_dict, num_neurons, num_layer_qwen05)\ntorch.save(overlap_dict, \"qwen05-overlap-across-dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T18:06:12.259934Z","iopub.execute_input":"2025-02-27T18:06:12.260371Z","iopub.status.idle":"2025-02-27T18:06:18.487785Z","shell.execute_reply.started":"2025-02-27T18:06:12.260334Z","shell.execute_reply":"2025-02-27T18:06:18.485726Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 🐸Gemma 2 on Massive and XCOPA","metadata":{}},{"cell_type":"code","source":"num_lang, _, _, num_neurons = full_languages_raw_values_massive_gemma.shape\n\nactivation_dict = get_k_lang_actv_dict(52, full_languages_raw_values_massive_gemma)\nmassive_gemma_dict = {i: activation_dict[k] for i, k in enumerate(id_match_massive) if k in activation_dict}\nactivation_dict = get_k_lang_actv_dict(11, full_languages_raw_values_xcopa_gemma)\nxcopa_gemma_dict = {i: activation_dict[k] for i, k in enumerate(id_match_xcopa) if k in activation_dict}\noverlap_dict = overlap_model_diff_dataset(massive_gemma_dict, xcopa_gemma_dict, num_neurons, num_layer_gemma2)\ntorch.save(overlap_dict, \"gemma2-overlap-across-dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T18:06:18.489702Z","iopub.execute_input":"2025-02-27T18:06:18.490059Z","iopub.status.idle":"2025-02-27T18:06:27.279120Z","shell.execute_reply.started":"2025-02-27T18:06:18.490030Z","shell.execute_reply":"2025-02-27T18:06:27.278011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_cpu_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T14:24:55.991681Z","iopub.status.idle":"2025-02-27T14:24:55.992250Z","shell.execute_reply":"2025-02-27T14:24:55.991976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport torch\nimport scipy.stats\n\n\ndef dist_matrix(tipe, tensor, num_lang, with_label, lang_dict, threshold):\n    \"\"\"\n    tipe = kl or spearman \n    tensor: shape(num_lang, 2, num_rows, num_neurons)\n    num_lang: num of languages\n    with_label: boolean\n    threshold: (float) e.g. 0.9, the threshold of correlation coeff. that determines if 2 languages overlap\n    lang_dict: (dict) e.g. {0: \"AF-za\", }\n    \"\"\"\n    matrix = torch.zeros(num_lang, num_lang)\n    if tipe == \"kl\":\n        activation_tensor = tensor.transpose(0, 1)[0].mean(dim=-2).clamp(min=0)\n        eps = 1e-6  # Small epsilon to avoid division by zero\n        \n        # Normalize each row to sum to 1 (convert to probability distributions)\n        row_sums = activation_tensor.sum(dim=1, keepdim=True)\n        activation_tensor = activation_tensor / (row_sums + eps)\n        \n        # Handle zero rows (replace with uniform distribution)\n        zero_rows = row_sums.squeeze() == 0\n        activation_tensor[zero_rows] = 1.0 / activation_tensor.shape[1]\n        \n        \n        for i in range(num_lang):\n            for j in range(num_lang):\n                matrix[i, j] = F.kl_div(\n                    activation_tensor[j].clamp(min=eps).log(),  # Q (log-probabilities)\n                    activation_tensor[i],  # P (reference distribution)\n                    reduction=\"batchmean\",  # Use batchmean instead of sum\n                    log_target=False  # P is not in log-space\n                )\n    elif tipe ==\"spearman_avg_on_activation_value\" : \n        activation_tensor = tensor.transpose(0,1)[0].mean(dim=-2)\n        \n        # Compute Spearman's rho for each pair of rows\n        for i in range(num_lang):\n            for j in range(num_lang):\n                rho, _ = scipy.stats.spearmanr(activation_tensor[i].cpu().numpy(), activation_tensor[j].cpu().numpy())\n                matrix[i, j] = rho  # Store result\n                \n    elif tipe == \"spearman_avg_on_corr_value\" :\n        activation_tensor = tensor.transpose(0,1)[0].transpose(0,1)\n        num_data_points = activation_tensor.size(0)\n        assert num_data_points == 200, \"not 200\"\n        #(num_data_points, num_lang, num_features)\n        \n        # Compute and accumulate Spearman correlation matrices\n        for data_idx in range(num_data_points):\n            activations = activation_tensor[data_idx].cpu().numpy()  # (num_lang, num_features)\n            \n            # Compute Spearman correlation matrix for this data point\n            corr_matrix, _ = scipy.stats.spearmanr(activations, axis=1)\n            \n            # Accumulate\n            matrix += corr_matrix\n        \n        # Average across all data points\n        matrix /= num_data_points   \n    else:\n        print(\"tipe doesnt exist\")\n        return\n\n    plt.figure(figsize=(10, 8))\n    if with_label:\n        sns.heatmap(matrix.numpy(), annot=True, fmt=\"d\", cmap=\"YlOrRd\", \n                    xticklabels=range(k+1), yticklabels=range(k+1))\n    else:\n        sns.heatmap(matrix, fmt=\"d\", cmap=\"YlOrRd\",cbar=True)\n    plt.xlabel(\"Key\")\n    plt.ylabel(\"Key\")\n    plt.title(f\"Overlap Heatmap of Activated Neurons: {tipe}\")\n    plt.show()\n\n    # if tipe is correlation, return dict of each languages' closest overlap\n    if tipe.startswith(\"spearman\"):\n        lang_correlation_dict = dict()\n        for i in range(num_lang):  # Iterate over each language (row index)\n            # Find indices where correlation >= threshold (excluding self)\n            mask = (matrix[i] >= threshold) & (torch.arange(num_lang) != i)\n            correlated_indices = torch.where(mask)[0]  # Indices of correlated languages\n            correlated_values = matrix[i, correlated_indices]  # Corresponding correlation values\n            correlated_list = list(zip(correlated_indices.tolist(), correlated_values.tolist()))\n            correlated_list.sort(key=lambda x: x[1], reverse=True)\n            # Convert to list of (index, correlation coefficient) tuples\n            lang = lang_dict[i] \n            lang_correlation_dict[lang] = correlated_list\n        return lang_correlation_dict\n                    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T15:07:07.512427Z","iopub.execute_input":"2025-02-27T15:07:07.512916Z","iopub.status.idle":"2025-02-27T15:07:07.530714Z","shell.execute_reply.started":"2025-02-27T15:07:07.512881Z","shell.execute_reply":"2025-02-27T15:07:07.529320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_corr(tensor, num_lang, lang_dict, threshold=0.7):\n    # dist_matrix(\"kl\", tensor, num_lang, False, lang_dict, threshold)\n    spearman1 = dist_matrix(\"spearman_avg_on_activation_value\", tensor, num_lang, False, lang_dict, threshold)\n    spearman2 = dist_matrix(\"spearman_avg_on_corr_value\", tensor, num_lang, False, lang_dict, threshold)\n    return spearman1, spearman2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T15:07:07.938030Z","iopub.execute_input":"2025-02-27T15:07:07.938457Z","iopub.status.idle":"2025-02-27T15:07:07.943892Z","shell.execute_reply.started":"2025-02-27T15:07:07.938418Z","shell.execute_reply":"2025-02-27T15:07:07.942673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸Massive Qwen","metadata":{}},{"cell_type":"code","source":"num_lang, _, num_rows, num_neurons = full_languages_raw_values.shape\ns1,s2 = visualize_corr(tensor=full_languages_raw_values, num_lang=num_lang, lang_dict=massive_lang_dict, threshold=0.7)\ntorch.save(s2, \"lang_overlap_massive_qwen05\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T15:07:11.094801Z","iopub.execute_input":"2025-02-27T15:07:11.095175Z","iopub.status.idle":"2025-02-27T15:13:38.472920Z","shell.execute_reply.started":"2025-02-27T15:07:11.095146Z","shell.execute_reply":"2025-02-27T15:13:38.471701Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸XCOPA Gemma","metadata":{}},{"cell_type":"code","source":"num_lang, _, num_rows, num_neurons = full_languages_raw_values_xcopa_gemma.shape\ns1,s2 = visualize_corr(tensor=full_languages_raw_values_xcopa_gemma, num_lang=num_lang, lang_dict=xcopa_lang_dict, threshold=0.7)\ntorch.save(s2, \"lang_overlap_xcopa_gemma2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:36:23.375984Z","iopub.execute_input":"2025-02-27T16:36:23.376465Z","iopub.status.idle":"2025-02-27T16:38:29.771524Z","shell.execute_reply.started":"2025-02-27T16:36:23.376428Z","shell.execute_reply":"2025-02-27T16:38:29.769989Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 🐸XCOPA Qwen","metadata":{}},{"cell_type":"code","source":"num_lang, _, num_rows, num_neurons = full_languages_raw_values_xcopa_qwen.shape\ns1,s2 = visualize_corr(tensor=full_languages_raw_values_xcopa_qwen, num_lang=num_lang, lang_dict=xcopa_lang_dict, threshold=0.7)\ntorch.save(s2, \"lang_overlap_xcopa_qwen05\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T15:15:42.536912Z","iopub.execute_input":"2025-02-27T15:15:42.537284Z","iopub.status.idle":"2025-02-27T15:16:41.834589Z","shell.execute_reply.started":"2025-02-27T15:15:42.537246Z","shell.execute_reply":"2025-02-27T15:16:41.833278Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 🐸MASSIVE Gemma","metadata":{}},{"cell_type":"code","source":"num_lang, _, num_rows, num_neurons = full_languages_raw_values_massive_gemma.shape\ns1,s2 = visualize_corr(tensor=full_languages_raw_values_massive_gemma, num_lang=num_lang, lang_dict=massive_lang_dict, threshold=0.7)\ntorch.save(s2, \"lang_overlap_massive_gemma2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T15:16:41.835830Z","iopub.execute_input":"2025-02-27T15:16:41.836125Z","iopub.status.idle":"2025-02-27T15:30:19.982166Z","shell.execute_reply.started":"2025-02-27T15:16:41.836101Z","shell.execute_reply":"2025-02-27T15:30:19.979898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lihat = torch.load(\"/kaggle/working/lang_overlap_massive_qwen05\", weights_only=False)\nlihat['id-ID']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:49:01.760135Z","iopub.execute_input":"2025-02-27T16:49:01.760659Z","iopub.status.idle":"2025-02-27T16:49:01.773265Z","shell.execute_reply.started":"2025-02-27T16:49:01.760617Z","shell.execute_reply":"2025-02-27T16:49:01.771719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lihat = torch.load(\"/kaggle/working/lang_overlap_massive_gemma2\", weights_only=False)\nlihat['id-ID']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:44:46.325416Z","iopub.execute_input":"2025-02-27T16:44:46.325956Z","iopub.status.idle":"2025-02-27T16:44:46.339357Z","shell.execute_reply.started":"2025-02-27T16:44:46.325920Z","shell.execute_reply":"2025-02-27T16:44:46.337750Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lihat = torch.load(\"/kaggle/working/lang_overlap_xcopa_gemma2\", weights_only=False)\n# lihat['id-ID'][:5]\nlihat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:54:23.781793Z","iopub.execute_input":"2025-02-27T16:54:23.782202Z","iopub.status.idle":"2025-02-27T16:54:23.794827Z","shell.execute_reply.started":"2025-02-27T16:54:23.782172Z","shell.execute_reply":"2025-02-27T16:54:23.793435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lihat = torch.load(\"/kaggle/working/lang_overlap_xcopa_qwen05\", weights_only=False)\n# lihat['id-ID'][:5]\nlihat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T16:54:03.002159Z","iopub.execute_input":"2025-02-27T16:54:03.002668Z","iopub.status.idle":"2025-02-27T16:54:03.015361Z","shell.execute_reply.started":"2025-02-27T16:54:03.002628Z","shell.execute_reply":"2025-02-27T16:54:03.014049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nos.environ['KAGGLE_KEY'] = user_secrets.get_secret(\"KAGGLE_KEY\")\nos.environ['KAGGLE_USERNAME'] = user_secrets.get_secret(\"KAGGLE_USERNAME\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T17:14:39.982776Z","iopub.execute_input":"2025-02-27T17:14:39.983222Z","iopub.status.idle":"2025-02-27T17:14:40.343556Z","shell.execute_reply.started":"2025-02-27T17:14:39.983191Z","shell.execute_reply":"2025-02-27T17:14:40.342255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.makedirs('/kaggle/dataset', exist_ok=True)\n\nos.makedirs('/kaggle/dataset/overlap-spearman', exist_ok=True)\n\n# Change below\nmeta = dict(\n    id=\"inayarhmns/overlap-spearman-langs-neurons\",\n    title=\"overlap-spearman-langs-neurons\",\n    isPrivate=True,\n    licenses=[dict(name=\"other\")]\n)\nwith open(f'/kaggle/dataset/overlap-spearman/dataset-metadata.json', 'w') as f:\n    json.dump(meta, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T17:16:28.233214Z","iopub.execute_input":"2025-02-27T17:16:28.233697Z","iopub.status.idle":"2025-02-27T17:16:28.242384Z","shell.execute_reply.started":"2025-02-27T17:16:28.233661Z","shell.execute_reply":"2025-02-27T17:16:28.241263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cp /kaggle/working/lang_overlap_massive_gemma2 /kaggle/dataset/overlap-spearman\n!cp /kaggle/working/lang_overlap_massive_qwen05 /kaggle/dataset/overlap-spearman\n!cp /kaggle/working/lang_overlap_xcopa_gemma2 /kaggle/dataset/overlap-spearman\n!cp /kaggle/working/lang_overlap_xcopa_qwen05 /kaggle/dataset/overlap-spearman\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T17:16:42.788423Z","iopub.execute_input":"2025-02-27T17:16:42.788916Z","iopub.status.idle":"2025-02-27T17:16:45.944733Z","shell.execute_reply.started":"2025-02-27T17:16:42.788878Z","shell.execute_reply":"2025-02-27T17:16:45.943072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(massive_lang_dict, \"/kaggle/dataset/overlap-spearman/massive_lang_dict\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T17:17:53.280692Z","iopub.execute_input":"2025-02-27T17:17:53.281198Z","iopub.status.idle":"2025-02-27T17:17:53.288048Z","shell.execute_reply.started":"2025-02-27T17:17:53.281157Z","shell.execute_reply":"2025-02-27T17:17:53.286840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(xcopa_lang_dict, \"/kaggle/dataset/overlap-spearman/xcopa_lang_dict\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T17:18:19.939726Z","iopub.execute_input":"2025-02-27T17:18:19.940204Z","iopub.status.idle":"2025-02-27T17:18:19.946763Z","shell.execute_reply.started":"2025-02-27T17:18:19.940169Z","shell.execute_reply":"2025-02-27T17:18:19.945168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -lh /kaggle/dataset/overlap-spearman\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T17:18:23.487634Z","iopub.execute_input":"2025-02-27T17:18:23.488050Z","iopub.status.idle":"2025-02-27T17:18:24.175521Z","shell.execute_reply.started":"2025-02-27T17:18:23.488017Z","shell.execute_reply":"2025-02-27T17:18:24.174067Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p \"/kaggle/dataset/overlap-spearman\" --dir-mode zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T17:19:42.049708Z","iopub.execute_input":"2025-02-27T17:19:42.050446Z","iopub.status.idle":"2025-02-27T17:19:50.906174Z","shell.execute_reply.started":"2025-02-27T17:19:42.050324Z","shell.execute_reply":"2025-02-27T17:19:50.903786Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"case 2","metadata":{}},{"cell_type":"markdown","source":"### 🐸Massive Qwen","metadata":{}},{"cell_type":"code","source":"num_lang, _, num_rows, num_neurons = full_languages_raw_values.shape\nvisualize_overlap(num_lang, full_languages_raw_values, method=\"topk\", topk=1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T04:51:16.972067Z","iopub.execute_input":"2025-02-21T04:51:16.972423Z","iopub.status.idle":"2025-02-21T04:51:20.719778Z","shell.execute_reply.started":"2025-02-21T04:51:16.972396Z","shell.execute_reply":"2025-02-21T04:51:20.718739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"TODO","metadata":{}},{"cell_type":"markdown","source":"### LAPE","metadata":{}},{"cell_type":"code","source":"def LAPE(activation_probs, top_rate = 0.01,filter_rate=0.95,activation_bar_ratio=0.95):\n    \"\"\"    \n    activation_probs = # layer x inter x lang_num\n    \"\"\"    \n    num_layers = activation_probs.size(0)\n    normed_activation_probs = activation_probs / activation_probs.sum(dim=-1, keepdim=True)\n    normed_activation_probs[torch.isnan(normed_activation_probs)] = 0\n    log_probs = torch.where(normed_activation_probs > 0, normed_activation_probs.log(), 0)\n    entropy = -torch.sum(normed_activation_probs * log_probs, dim=-1)\n    largest = False\n    \n    if torch.isnan(entropy).sum():\n        print(torch.isnan(entropy).sum())\n        raise ValueError\n    \n    flattened_probs = activation_probs.flatten()\n    top_prob_value = flattened_probs.kthvalue(round(len(flattened_probs) * filter_rate)).values.item()\n    print(top_prob_value)\n    # dismiss the neruon if no language has an activation value over top 90%\n    top_position = (activation_probs > top_prob_value).sum(dim=-1)\n    entropy[top_position == 0] = -torch.inf if largest else torch.inf\n\n    flattened_entropy = entropy.flatten()\n    top_entropy_value = round(len(flattened_entropy) * top_rate)\n    _, index = flattened_entropy.topk(top_entropy_value, largest=largest)\n    row_index = index // entropy.size(1)\n    col_index = index % entropy.size(1)\n    selected_probs = activation_probs[row_index, col_index] # n x lang\n    # for r, c in zip(row_index, col_index):\n    #     print(r, c, activation_probs[r][c])\n\n    # print(selected_probs.size(0), torch.bincount(selected_probs.argmax(dim=-1)))\n    selected_probs = selected_probs.transpose(0, 1)\n    activation_bar = flattened_probs.kthvalue(round(len(flattened_probs) * activation_bar_ratio)).values.item()\n    # print((selected_probs > activation_bar).sum(dim=1).tolist())\n    lang, indice = torch.where(selected_probs > activation_bar)\n\n    merged_index = torch.stack((row_index, col_index), dim=-1)\n    final_indice = []\n    for _, index in enumerate(indice.split(torch.bincount(lang).tolist())):\n        lang_index = [tuple(row.tolist()) for row in merged_index[index]]\n        lang_index.sort()\n        layer_index = [[] for _ in range(num_layers)]\n        for l, h in lang_index:\n            layer_index[l].append(h)\n        for l, h in enumerate(layer_index):\n            layer_index[l] = torch.tensor(h).long()\n        final_indice.append(layer_index)\n    return final_indice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:37:06.787375Z","iopub.execute_input":"2025-02-24T01:37:06.787643Z","iopub.status.idle":"2025-02-24T01:37:06.802185Z","shell.execute_reply.started":"2025-02-24T01:37:06.787610Z","shell.execute_reply":"2025-02-24T01:37:06.801413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_prob_for_lape(tensor, num_lang, num_layer, num_neurons):\n    \"\"\"\n    num_neurons: intermediate layer (neurons in a layer)\n    \"\"\"\n    full_languages_raw_values = (tensor.transpose(0,1)[0] > 0).half()\n    probs = full_languages_raw_values.mean(dim=-2)\n    probs.shape\n    del full_languages_raw_values\n    probs = probs.reshape(num_lang,num_layer,num_neurons)\n    transposed_probs = probs.transpose(0,1).transpose(-1,-2)\n    transposed_probs.shape\n    return transposed_probs\n\ndef convert_to_global_indices(final_indice, intermediate_size):\n    final_flattened = []\n    \n    for lang_idx, layers in enumerate(final_indice):  # Iterate over languages\n        global_indices = []\n        for layer_idx, hidden_units in enumerate(layers):  # Iterate over layers\n            if hidden_units.numel() > 0:  # If there are selected neurons\n                global_indices.extend((layer_idx * intermediate_size + hidden_units).tolist())\n        \n        final_flattened.append((global_indices))  \n    \n    return final_flattened\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:37:06.802850Z","iopub.execute_input":"2025-02-24T01:37:06.803074Z","iopub.status.idle":"2025-02-24T01:37:06.819830Z","shell.execute_reply.started":"2025-02-24T01:37:06.803054Z","shell.execute_reply":"2025-02-24T01:37:06.819087Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef visualize_lape(full_languages_raw_values, num_layer, top_rate = 0.05,filter_rate=0.80,activation_bar_ratio=0.80): \n    num_lang, _, num_rows, num_neurons = full_languages_raw_values.shape\n    \n    per_layer = int(num_neurons/num_layer)\n    transposed_probs = get_prob_for_lape(full_languages_raw_values, num_lang, num_layer, per_layer)\n    lape = LAPE(transposed_probs, top_rate = top_rate,filter_rate=filter_rate,activation_bar_ratio=activation_bar_ratio)\n    flattened_indices = convert_to_global_indices(lape, per_layer)\n    activation_dict = dict()\n    for i in range (num_lang):\n        activation_dict[i] = flattened_indices[i]\n    make_heatmap_neuron_overlap(activation_dict, num_lang, False, \"jaccard\", 1)\n    return lape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:41:08.890808Z","iopub.execute_input":"2025-02-24T01:41:08.891133Z","iopub.status.idle":"2025-02-24T01:41:08.896882Z","shell.execute_reply.started":"2025-02-24T01:41:08.891109Z","shell.execute_reply":"2025-02-24T01:41:08.895889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸Massive Qwen","metadata":{}},{"cell_type":"markdown","source":"Jaccard similarity LAPE neurons. Sedikit yang overlap karena LAPE language-specific dan kemungkinannya kecil untuk sharing between neurons","metadata":{}},{"cell_type":"code","source":"lape_massive_qwen = visualize_lape(full_languages_raw_values, 24, top_rate = 0.02,filter_rate=0.80,activation_bar_ratio=0.80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:54:54.746662Z","iopub.execute_input":"2025-02-24T01:54:54.746964Z","iopub.status.idle":"2025-02-24T01:55:01.099811Z","shell.execute_reply.started":"2025-02-24T01:54:54.746940Z","shell.execute_reply":"2025-02-24T01:55:01.098546Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸XCOPA GEMMA","metadata":{}},{"cell_type":"code","source":"lape_xcopa_gemma = visualize_lape(full_languages_raw_values_xcopa_gemma, num_layer=26, top_rate = 0.01,filter_rate=0.80,activation_bar_ratio=0.80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:42:18.426453Z","iopub.execute_input":"2025-02-24T01:42:18.426799Z","iopub.status.idle":"2025-02-24T01:42:19.217143Z","shell.execute_reply.started":"2025-02-24T01:42:18.426769Z","shell.execute_reply":"2025-02-24T01:42:19.216346Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### intervention LAPE\n","metadata":{}},{"cell_type":"code","source":"import psutil\n\ndef check_cpu_memory():\n    mem = psutil.virtual_memory()\n    print(f\"Total Memory: {mem.total / 1024**3:.2f} GB\")\n    print(f\"Available Memory: {mem.available / 1024**3:.2f} GB\")\n    print(f\"Used Memory: {mem.used / 1024**3:.2f} GB\")\n    print(f\"Memory Usage: {mem.percent}%\")\n\nimport torch\n\ndef clear_cuda_cache():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n\nclear_cuda_cache()\ncheck_cpu_memory()\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:37:14.719141Z","iopub.execute_input":"2025-02-24T01:37:14.719413Z","iopub.status.idle":"2025-02-24T01:37:15.248769Z","shell.execute_reply.started":"2025-02-24T01:37:14.719392Z","shell.execute_reply":"2025-02-24T01:37:15.247684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndef clear_cuda_cache():\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n\nclear_cuda_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T00:26:21.798381Z","iopub.execute_input":"2025-02-21T00:26:21.798719Z","iopub.status.idle":"2025-02-21T00:26:21.920463Z","shell.execute_reply.started":"2025-02-21T00:26:21.798690Z","shell.execute_reply":"2025-02-21T00:26:21.919440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_hooks(infer_model):\n    for i in range(len(infer_model.model.model.layers)):\n        mlp = infer_model.model.model.layers[i].mlp\n        mlp.act_fn._forward_hooks.clear()\ndef test_inference(infer_model, prompt, max_new_tokens):\n    generated_text = infer_model.inference(prompt, max_new_tokens)\n    # print(f\"data: {prompt}\")\n            \n    print(f\"gen_text {generated_text}\")\ndef set_activation_mlp(name, lape_lang, is_multiplied, replace_value): \n    \"\"\"\n        name (str): buat namain layer\n        lape_lang: lape untuk suatu lang\n        is_multiplied: (bool) true if multiplied else replaced\n    \"\"\"\n    def hook_fn(module, input, output):\n        # print(f\"output {output.shape}\")\n        layer = int(name)\n        if is_multiplied:\n            output[0, :, lape_lang[layer]] *= replace_value\n        else:\n            output[0, :, lape_lang[layer]] = replace_value\n    return hook_fn\n\n\ndef intervensi(prompt, infer_model, lape_lang, num_layers, max_new_tokens, is_multiplied, replace_value):\n    clean_hooks(infer_model)\n    handlers = []\n    for i in range (num_layers):\n        mlp = infer_model.model.model.layers[i].mlp\n        handlers.append(mlp.act_fn.register_forward_hook(set_activation_mlp(f\"{i}\", lape_lang, is_multiplied, replace_value)))\n    test_inference(infer_model, prompt, max_new_tokens)\n    for i in handlers:\n        i.remove()\n    clean_hooks(infer_model)\n\ndef intervene_langs(infer_model, prompt_lang, lape_langs, num_layers, max_new_tokens=10, is_multiplied=False, replace_value=0):\n    \"\"\"\n    given a prompt, from number of languages, see how intervention each language affects the prompt.\n    infer_model: InferenceModel\n    prompt_lang: (str) prompt dalam bahasa tertentu\n    lape_langs: (dict) key: lang, value: lape in a language\n    \"\"\"\n    print(f\"original: \")\n    test_inference(infer_model, prompt_lang, max_new_tokens)\n    for key, value in lape_langs.items():\n        print(f\"intervensi {key}\")\n        intervensi(prompt_lang, infer_model, value, num_layers, max_new_tokens, is_multiplied, replace_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:41:17.608145Z","iopub.execute_input":"2025-02-24T01:41:17.608490Z","iopub.status.idle":"2025-02-24T01:41:17.616598Z","shell.execute_reply.started":"2025-02-24T01:41:17.608460Z","shell.execute_reply":"2025-02-24T01:41:17.615541Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸Massive Qwen","metadata":{}},{"cell_type":"code","source":"infer_model = massive_qwen\nnum_layers = 24","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:54:33.345100Z","iopub.execute_input":"2025-02-24T01:54:33.345621Z","iopub.status.idle":"2025-02-24T01:54:33.350714Z","shell.execute_reply.started":"2025-02-24T01:54:33.345575Z","shell.execute_reply":"2025-02-24T01:54:33.349624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lape_langs = {\n    \"indo\" : lape_massive_qwen[19],\n    \"malay\" : lape_massive_qwen[31],\n    'java' : lape_massive_qwen[23],\n    'eng': lape_massive_qwen[10]\n}\nprompt_indo = \"batalkan alarm saya pukul tujuh pagi\"\nintervene_langs(\n    infer_model, prompt_indo, lape_langs, num_layers, max_new_tokens=10, is_multiplied=False, replace_value=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:54:36.980685Z","iopub.execute_input":"2025-02-24T01:54:36.981080Z","iopub.status.idle":"2025-02-24T01:54:38.398345Z","shell.execute_reply.started":"2025-02-24T01:54:36.981040Z","shell.execute_reply":"2025-02-24T01:54:38.397397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lape_langs = {\n    \"indo\" : lape_massive_qwen[19],\n    \"malay\" : lape_massive_qwen[31],\n    'java' : lape_massive_qwen[23],\n    'eng': lape_massive_qwen[10]\n}\nprompt_indo = \"batalkan alarm saya pukul tujuh pagi\"\nintervene_langs(\n    infer_model, prompt_indo, lape_langs, num_layers, max_new_tokens=10, is_multiplied=False, replace_value=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T01:43:51.270231Z","iopub.execute_input":"2025-02-21T01:43:51.270622Z","iopub.status.idle":"2025-02-21T01:43:52.842844Z","shell.execute_reply.started":"2025-02-21T01:43:51.270591Z","shell.execute_reply":"2025-02-21T01:43:52.841651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_indo = \"batalkan alarm saya pukul tujuh pagi\"\nintervene_langs(\n    infer_model, prompt_indo, lape_langs, num_layers, max_new_tokens=10, is_multiplied=False, replace_value=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T01:44:12.652733Z","iopub.execute_input":"2025-02-21T01:44:12.653082Z","iopub.status.idle":"2025-02-21T01:44:14.248406Z","shell.execute_reply.started":"2025-02-21T01:44:12.653056Z","shell.execute_reply":"2025-02-21T01:44:14.247451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_indo = \"batalkan alarm saya pukul tujuh pagi\"\nintervene_langs(\n    massive_qwen, prompt_indo, lape_langs, num_layers, max_new_tokens=10, is_multiplied=True, replace_value=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T01:44:14.249470Z","iopub.execute_input":"2025-02-21T01:44:14.249724Z","iopub.status.idle":"2025-02-21T01:44:15.833572Z","shell.execute_reply.started":"2025-02-21T01:44:14.249703Z","shell.execute_reply":"2025-02-21T01:44:15.832575Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 🐸XCOPA Gemma","metadata":{}},{"cell_type":"code","source":"# lape_massive_qwen","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:40:40.418342Z","iopub.execute_input":"2025-02-24T01:40:40.418671Z","iopub.status.idle":"2025-02-24T01:40:40.422296Z","shell.execute_reply.started":"2025-02-24T01:40:40.418646Z","shell.execute_reply":"2025-02-24T01:40:40.421384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"infer_model = xcopa_gemma\nnum_layers = 26","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:42:27.034951Z","iopub.execute_input":"2025-02-24T01:42:27.035258Z","iopub.status.idle":"2025-02-24T01:42:27.039222Z","shell.execute_reply.started":"2025-02-24T01:42:27.035234Z","shell.execute_reply":"2025-02-24T01:42:27.038225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_hooks(xcopa_gemma)\nxcopa_gemma.model.model.layers[0].mlp.act_fn._forward_hooks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:42:27.564448Z","iopub.execute_input":"2025-02-24T01:42:27.564764Z","iopub.status.idle":"2025-02-24T01:42:27.570512Z","shell.execute_reply.started":"2025-02-24T01:42:27.564740Z","shell.execute_reply":"2025-02-24T01:42:27.569681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lape_langs = {\n    \"indo\" : lape_xcopa_gemma[2],\n    \"haitian\" : lape_xcopa_gemma[1],\n    'chinese' : lape_xcopa_gemma[10],\n    'swahili': lape_xcopa_gemma[5]\n}\nprompt_indo = \"batalkan alarm saya pukul tujuh pagi\"\nintervene_langs(\n    xcopa_gemma, prompt_indo, lape_langs, num_layers, max_new_tokens=20, is_multiplied=False, replace_value=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:42:29.115733Z","iopub.execute_input":"2025-02-24T01:42:29.116065Z","iopub.status.idle":"2025-02-24T01:42:34.753118Z","shell.execute_reply.started":"2025-02-24T01:42:29.116038Z","shell.execute_reply":"2025-02-24T01:42:34.752361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_hooks(xcopa_gemma)\nlape_langs = {\n    \"indo\" : lape_xcopa_gemma[2],\n    \"haitian\" : lape_xcopa_gemma[1],\n    'chinese' : lape_xcopa_gemma[10],\n    'swahili': lape_xcopa_gemma[5]\n}\nprompt_indo = \"batalkan alarm saya pukul tujuh pagi\"\nintervene_langs(\n    xcopa_gemma, prompt_indo, lape_langs, num_layers, max_new_tokens=20, is_multiplied=True, replace_value=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T01:43:27.030391Z","iopub.execute_input":"2025-02-24T01:43:27.030736Z","iopub.status.idle":"2025-02-24T01:43:32.571489Z","shell.execute_reply.started":"2025-02-24T01:43:27.030707Z","shell.execute_reply":"2025-02-24T01:43:32.570692Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"=================================","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}