{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10856395,"sourceType":"datasetVersion","datasetId":6743330},{"sourceId":10868887,"sourceType":"datasetVersion","datasetId":6752292},{"sourceId":10946309,"sourceType":"datasetVersion","datasetId":6808469}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\nfrom datasets import load_dataset\nimport pandas as pd\nimport torch\ntorch.manual_seed(42)\ntorch.cuda.manual_seed_all(42)\nset_seed(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"ha\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(\"***REMOVED***\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model_name = \"Qwen/Qwen2.5-3B-Instruct\"\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\n# model_name = \"google/gemma-2-2b-it\"\n# file_qwen05_massive = \"/kaggle/input/raw-qwen3-predictv1/raw_qwen3_predict_massive200.pt\" #ini 3b\nfile_qwen05_massive = \"/kaggle/input/raw-qwen05-predict-200/raw_qwen05_predict_massive200_selected_v.pt\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LAPE","metadata":{}},{"cell_type":"code","source":"def LAPE(activation_probs, top_rate = 0.01,filter_rate=0.95,activation_bar_ratio=0.95):\n    \"\"\"    \n    activation_probs = # layer x inter x lang_num\n    \"\"\"    \n    num_layers = activation_probs.size(0)\n    normed_activation_probs = activation_probs / activation_probs.sum(dim=-1, keepdim=True)\n    normed_activation_probs[torch.isnan(normed_activation_probs)] = 0\n    log_probs = torch.where(normed_activation_probs > 0, normed_activation_probs.log(), 0)\n    entropy = -torch.sum(normed_activation_probs * log_probs, dim=-1)\n    largest = False\n    \n    if torch.isnan(entropy).sum():\n        print(torch.isnan(entropy).sum())\n        raise ValueError\n    \n    flattened_probs = activation_probs.flatten()\n    top_prob_value = flattened_probs.kthvalue(round(len(flattened_probs) * filter_rate)).values.item()\n    print(top_prob_value)\n    # dismiss the neruon if no language has an activation value over top 90%\n    top_position = (activation_probs > top_prob_value).sum(dim=-1)\n    entropy[top_position == 0] = -torch.inf if largest else torch.inf\n\n    flattened_entropy = entropy.flatten()\n    top_entropy_value = round(len(flattened_entropy) * top_rate)\n    _, index = flattened_entropy.topk(top_entropy_value, largest=largest)\n    row_index = index // entropy.size(1)\n    col_index = index % entropy.size(1)\n    selected_probs = activation_probs[row_index, col_index] # n x lang\n    # for r, c in zip(row_index, col_index):\n    #     print(r, c, activation_probs[r][c])\n\n    # print(selected_probs.size(0), torch.bincount(selected_probs.argmax(dim=-1)))\n    selected_probs = selected_probs.transpose(0, 1)\n    activation_bar = flattened_probs.kthvalue(round(len(flattened_probs) * activation_bar_ratio)).values.item()\n    # print((selected_probs > activation_bar).sum(dim=1).tolist())\n    lang, indice = torch.where(selected_probs > activation_bar)\n\n    merged_index = torch.stack((row_index, col_index), dim=-1)\n    final_indice = []\n    for _, index in enumerate(indice.split(torch.bincount(lang).tolist())):\n        lang_index = [tuple(row.tolist()) for row in merged_index[index]]\n        lang_index.sort()\n        layer_index = [[] for _ in range(num_layers)]\n        for l, h in lang_index:\n            layer_index[l].append(h)\n        for l, h in enumerate(layer_index):\n            layer_index[l] = torch.tensor(h).long()\n        final_indice.append(layer_index)\n    return final_indice\n\ndef get_prob_for_lape(tensor, num_lang, num_layer, num_neurons):\n    \"\"\"\n    num_neurons: intermediate layer (neurons in a layer)\n    \"\"\"\n    full_languages_raw_values = (tensor.transpose(0,1)[0] > 0).half()\n    probs = full_languages_raw_values.mean(dim=-2)\n    probs.shape\n    del full_languages_raw_values\n    probs = probs.reshape(num_lang,num_layer,num_neurons)\n    transposed_probs = probs.transpose(0,1).transpose(-1,-2)\n    transposed_probs.shape\n    return transposed_probs\n\ndef convert_to_global_indices(final_indice, intermediate_size):\n    final_flattened = []\n    \n    for lang_idx, layers in enumerate(final_indice):  # Iterate over languages\n        global_indices = []\n        for layer_idx, hidden_units in enumerate(layers):  # Iterate over layers\n            if hidden_units.numel() > 0:  # If there are selected neurons\n                global_indices.extend((layer_idx * intermediate_size + hidden_units).tolist())\n        \n        final_flattened.append((global_indices))  \n    \n    return final_flattened\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom scipy.stats import entropy\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\ndef get_k_lang_actv_dict(k, full_neurons, method=\"default\", topk=0):\n    \n    \"\"\"\n    di sini anggap activated neuron (avg token) yang di setiap row di dataset(cuman 2 row) actv valuenya > 0.\n    k = num language\"\"\"\n    activation_dict = {}\n    \n    full_neurons = full_neurons.transpose(-3,-4)[0].transpose(-1,-2)\n\n    if method == \"default\":\n        for i in range (full_neurons.size(0)):\n            tensor_lang = full_neurons[i]\n            rows_with_both_positive = (tensor_lang > 0).all(dim=-1)\n            \n            indices = torch.where(rows_with_both_positive)[0]\n            activation_dict[i] = indices\n            # indices.shape\n            # print(indices.shape)\n    elif method == \"topk\":\n        if topk==0:\n            print(f\"topk must not be 0\")\n        top = (full_neurons.mean(dim=-1).topk(topk).indices)\n        for i in range (full_neurons.size(0)):\n            activation_dict[i] = top[i]\n    \n    return activation_dict\n\n# Step 1: Convert lists to probability distributions\ndef to_probability_distribution(values):\n    total = sum(values)\n    return [v / total for v in values]\n\n\ndef make_heatmap_neuron_overlap(activation_dict, k, with_label=True, method=\"default\", alpha=1, normalized =False):    \n    # Example dictionary: keys 0-52, values are 1D tensors of activated neuron indices\n    # activation_dict = get_k_lang_actv_dict(10)\n\n    overlap_matrix = torch.tensor([])\n    if method == \"default\":\n        # Step 1: Create a binary matrix\n        max_neuron_index = max(max(indices) for indices in activation_dict.values()) + 1  # Find the maximum neuron index\n        binary_matrix = torch.zeros((k+1, max_neuron_index), dtype=torch.int)  # Initialize binary matrix\n        \n        for key, indices in activation_dict.items():\n            binary_matrix[key, indices] = 1  # Set activated neurons to 1\n        \n        # Step 2: Compute overlaps (dot product between rows)\n        overlap_matrix = torch.matmul(binary_matrix, binary_matrix.T)  # Dot product of binary_matrix with its transpose\n\n    elif method == \"jaccard\":\n        max_neuron_index = max(max(indices) for indices in activation_dict.values()) + 1\n        binary_matrix = torch.zeros((k+1, max_neuron_index), dtype=torch.int)\n    \n        # Fill binary matrix with activation data\n        for key, indices in activation_dict.items():\n            binary_matrix[key, indices] = 1  \n    \n        # Compute Jaccard distance matrix\n        overlap_matrix = torch.zeros((k+1, k+1))\n    \n        for i in range(k+1):\n            for j in range(k+1):\n                intersection = (binary_matrix[i] & binary_matrix[j]).sum().item()\n                union = (binary_matrix[i] | binary_matrix[j]).sum().item()\n                jaccard_similarity = intersection / union if union > 0 else 0\n                overlap_matrix[i, j] = jaccard_similarity\n        overlap_matrix = overlap_matrix ** alpha\n        if normalized:\n            overlap_matrix = overlap_matrix / overlap_matrix.sum(axis=1, keepdims=True)\n\n    \n    # Step 3: Visualize the heatmap\n    plt.figure(figsize=(10, 8))\n    if with_label:\n        sns.heatmap(overlap_matrix.numpy(), annot=True, fmt=\"d\", cmap=\"YlOrRd\", \n                    xticklabels=range(k+1), yticklabels=range(k+1))\n    else:\n        sns.heatmap(overlap_matrix.numpy(), fmt=\"d\", cmap=\"YlOrRd\",cbar=True)\n    plt.xlabel(\"Key\")\n    plt.ylabel(\"Key\")\n    plt.title(f\"Overlap Heatmap of Activated Neurons: {method}\")\n    plt.show()\n    return overlap_matrix\n\n\ndef normed_heatmap_neuron_overlap(num_lang, activation_dict):\n    # Step 1: Create a binary matrix\n    max_neuron_index = max(max(indices) for indices in activation_dict.values()) + 1  # Find the maximum neuron index\n    binary_matrix = torch.zeros((num_lang, max_neuron_index), dtype=torch.int)  # Initialize binary matrix\n    \n    for key, indices in activation_dict.items():\n        binary_matrix[key, indices] = 1  # Set activated neurons to 1\n    \n    # Step 2: Compute overlaps (dot product between rows)\n    overlap_matrix = torch.matmul(binary_matrix, binary_matrix.T)  # Dot product of binary_matrix with its transpose\n    \n    # Step 3: Normalize the overlap matrix\n    # Compute the number of activated neurons for each key\n    num_activated_neurons = binary_matrix.sum(dim=1, keepdim=True)\n    \n    # Normalize by the minimum number of activated neurons between each pair of keys\n    normalized_overlap_matrix = overlap_matrix / torch.minimum(\n        num_activated_neurons, num_activated_neurons.T\n    )\n    \n    # Ensure the diagonal is exactly 1 (self-overlap is always 1)\n    normalized_overlap_matrix.fill_diagonal_(1)\n    \n    # Step 4: Visualize the normalized heatmap (no labels or annotations)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(normalized_overlap_matrix.numpy(), cmap=\"YlOrRd\", cbar=True, vmin=0, vmax=1)\n    plt.xlabel(\"Key\")\n    plt.ylabel(\"Key\")\n    plt.title(\"Normalized Overlap Heatmap of Activated Neurons\")\n    plt.show()\n\n\ndef visualize_lape(full_languages_raw_values, num_layer, top_rate = 0.05,filter_rate=0.80,activation_bar_ratio=0.80): \n    num_lang, _, num_rows, num_neurons = full_languages_raw_values.shape\n    \n    per_layer = int(num_neurons/num_layer)\n    transposed_probs = get_prob_for_lape(full_languages_raw_values, num_lang, num_layer, per_layer)\n    lape = LAPE(transposed_probs, top_rate = top_rate,filter_rate=filter_rate,activation_bar_ratio=activation_bar_ratio)\n    flattened_indices = convert_to_global_indices(lape, per_layer)\n    activation_dict = dict()\n    for i in range (num_lang):\n        activation_dict[i] = flattened_indices[i]\n    make_heatmap_neuron_overlap(activation_dict, num_lang, False, \"jaccard\", 1)\n    return lape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"languages = {\n    0: 'Afrikaans (af-ZA)',\n    1: 'Amharic (am-ET)',\n    2: 'Arabic (ar-SA)',\n    3: 'Azerbaijani (az-AZ)',\n    4: 'Bengali (bn-BD)',\n    5: 'Catalan (ca-ES)',\n    6: 'Welsh (cy-GB)',\n    7: 'Danish (da-DK)',\n    8: 'German (de-DE)',\n    9: 'Greek (el-GR)',\n    10: 'English (en-US)',\n    11: 'Spanish (es-ES)',\n    12: 'Persian (fa-IR)',\n    13: 'Finnish (fi-FI)',\n    14: 'French (fr-FR)',\n    15: 'Hebrew (he-IL)',\n    16: 'Hindi (hi-IN)',\n    17: 'Hungarian (hu-HU)',\n    18: 'Armenian (hy-AM)',\n    19: 'Indonesian (id-ID)',\n    20: 'Icelandic (is-IS)',\n    21: 'Italian (it-IT)',\n    22: 'Japanese (ja-JP)',\n    23: 'Javanese (jv-ID)',\n    24: 'Georgian (ka-GE)',\n    25: 'Khmer (km-KH)',\n    26: 'Kannada (kn-IN)',\n    27: 'Korean (ko-KR)',\n    28: 'Latvian (lv-LV)',\n    29: 'Malayalam (ml-IN)',\n    30: 'Mongolian (mn-MN)',\n    31: 'Malay (ms-MY)',\n    32: 'Burmese (my-MM)',\n    33: 'Norwegian Bokmål (nb-NO)',\n    34: 'Dutch (nl-NL)',\n    35: 'Polish (pl-PL)',\n    36: 'Portuguese (pt-PT)',\n    37: 'Romanian (ro-RO)',\n    38: 'Russian (ru-RU)',\n    39: 'Slovenian (sl-SL)',\n    40: 'Albanian (sq-AL)',\n    41: 'Swedish (sv-SE)',\n    42: 'Swahili (sw-KE)',\n    43: 'Tamil (ta-IN)',\n    44: 'Telugu (te-IN)',\n    45: 'Thai (th-TH)',\n    46: 'Tagalog (tl-PH)',\n    47: 'Turkish (tr-TR)',\n    48: 'Urdu (ur-PK)',\n    49: 'Vietnamese (vi-VN)',\n    50: 'Chinese (Simplified) (zh-CN)',\n    51: 'Chinese (Traditional) (zh-TW)'\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfull_languages_raw_values = torch.load(file_qwen05_massive, weights_only=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lape_massive_qwen_2 = visualize_lape(full_languages_raw_values, 24, top_rate = 0.02,filter_rate=0.80,activation_bar_ratio=0.80)\n# lape_langs = {key:lape_massive_qwen_2[key] for key, value in enumerate(lape_massive_qwen_2)}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# lape_langs = {key:lape_massive_qwen_2[key] for key, value in enumerate(lape_massive_qwen_2)}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(42)\ntorch.cuda.manual_seed_all(42)\nset_seed(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nclass InferenceModel:\n    def __init__(self, model_name, device):\n        self.model_name = model_name\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", offload_buffers=True)\n    def inference(self, prompt, max_new_tokens=1):\n        \"\"\"\n        Performs inference on a given prompt.\n        Returns the decodede output\n        \"\"\"\n        generated_text = ''\n        len_sentence = 0\n        if self.model_name == 'Qwen/Qwen2.5-0.5B-Instruct' or self.model_name == 'Qwen/Qwen2.5-3B-Instruct':\n            # WARNING: messages ini harus sama dengan messages di check_index_prompt\n            messages = [\n            {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": prompt}\n            ]\n            text = self.tokenizer.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True\n            )\n            model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n            \n            generated_ids = self.model.generate(\n                    **model_inputs,\n                    do_sample=False,\n                    temperature=None,\n                    top_p=None,\n                    top_k=None,\n                    max_new_tokens=max_new_tokens\n                )\n            generated_ids = [\n                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n            ]\n            generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n            len_sentence = len(self.tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)[\"input_ids\"])\n        elif self.model_name == \"google/gemma-2-2b-it\":\n            # WARNING: messages ini harus sama dengan messages di check_index_prompt\n            messages = [\n            {\"role\": \"user\", \"content\": prompt}\n            ]\n            text = self.tokenizer.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True\n            )\n            model_inputs = self.tokenizer([text], return_tensors=\"pt\", add_special_tokens=False).to(self.model.device)\n            # print(f\"(model_inputs['input_ids'] {(model_inputs['input_ids'])}\")\n            \n            # print(f\"len(model_inputs['input_ids'] {len(model_inputs['input_ids'])}\")\n            sentence_token_texts = self.tokenizer.convert_ids_to_tokens((model_inputs['input_ids'])[0])\n            # print(f\"sentence_token_texts {sentence_token_texts}\")\n            \n            \n            generated_ids = self.model.generate(\n                    **model_inputs,\n                    do_sample=False,\n                    temperature=None,\n                    top_p=None,\n                    top_k=None,\n                    max_new_tokens=max_new_tokens\n                )\n            generated_ids = [\n                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n            ]\n            generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n            len_sentence = len(self.tokenizer(text, return_offsets_mapping=True, add_special_tokens=False)[\"input_ids\"])\n            # print()\n            \n        else:\n            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n            len_sentence = len(inputs[\"input_ids\"])\n            output = self.model.generate(\n                **inputs, \n                do_sample=False,\n                temperature=None,\n                top_p=None,\n                top_k=None,\n                max_new_tokens=1)\n            # print(f\"output: {output}\")\n            generated_ids = [\n                output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output)\n            ]\n            generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n        # answer = \"1\" if \"option1\" in generated_text else \"2\" if \"option2\" in generated_text else None\n        \n        return generated_text, len_sentence\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = InferenceModel(model_name, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_hooks(infer_model):\n    for i in range(len(infer_model.model.model.layers)):\n        mlp = infer_model.model.model.layers[i].mlp\n        mlp.act_fn._forward_hooks.clear()\ndef test_inference(infer_model, prompt, max_new_tokens):\n    generated_text = infer_model.inference(prompt, max_new_tokens)\n    # print(f\"data: {prompt}\")\n            \n    print(f\"gen_text {generated_text}\")\ndef set_activation_mlp(model_name, name, lape_lang, operation): \n    \"\"\"\n        name (str): buat namain layer\n        lape_lang: lape untuk suatu lang\n        operation: (bool) true if multiplied else replaced\n    \"\"\"\n    operand = operation[0]\n    replace_value = int(operation[1:])\n    if operand not in [\"=\", \"*\", \"+\"]:\n        raise ValueError(\"operand is wrong!\")\n\n    # TODO: coba di bagian promptnya aja jangan chat template\n    def hook_fn(module, input, output):\n        # print(f\"output {output.shape}\")\n        start_id_to_intv = -3 if model_name == \"Qwen/Qwen2.5-0.5B-Instruct\" or model_name == 'Qwen/Qwen2.5-3B-Instruct' else 0 \n        layer = int(name)\n        if operand == \"*\":\n            output[0, start_id_to_intv:, lape_lang[layer]] *= replace_value\n        elif operand == \"=\":\n            output[0, start_id_to_intv:, lape_lang[layer]] = replace_value\n        else:\n            output[0, start_id_to_intv:, lape_lang[layer]] += replace_value\n    return hook_fn\ndef set_activation_mlp_v2(replace_method, model_name, name, lape_langs, target_lang, operation_non_target, operation_target): \n    \"\"\"\n    This changes all neuron lape values to be replaced_values but leave behind a desired target language. \n        replace_method: lape or all\n        name (str): buat namain layer\n        lape_langs: dict semua language lape\n        target_lang: int index target_lang in lape\n        operation: *10, =0, +5, etc\n    \"\"\"\n    operand_t = operation_target[0]\n    replace_value_t = int(operation_target[1:])\n    if operand_t not in [\"=\", \"*\", \"+\"]:\n        raise ValueError(\"operand is wrong!\")\n\n    operand_nt = operation_non_target[0]\n    replace_value_nt = int(operation_non_target[1:])\n    if operand_nt not in [\"=\", \"*\", \"+\"]:\n        raise ValueError(\"operand is wrong!\")\n\n    # TODO: coba di bagian promptnya aja jangan chat template\n    def hook_fn(module, input, output):\n        # print(f\"output {output.shape}\")\n        start_id_to_intv = -3 if model_name == \"Qwen/Qwen2.5-0.5B-Instruct\" or model_name == 'Qwen/Qwen2.5-3B-Instruct' else 0 \n        layer = int(name)\n        if replace_method == \"lape\":\n            for lang, lape_lang in lape_langs.items():\n                if lang == target_lang:\n                    if operand_t == \"*\":\n                        output[0, start_id_to_intv:, lape_lang[layer]] *= replace_value_t\n                    elif operand_t == \"=\":\n                        output[0, start_id_to_intv:, lape_lang[layer]] = replace_value_t\n                    else:\n                        output[0, start_id_to_intv:, lape_lang[layer]] += replace_value_t\n                else:\n                    if operand_nt == \"*\":\n                        output[0, start_id_to_intv:, lape_lang[layer]] *= replace_value_nt\n                    elif operand_nt == \"=\":\n                        output[0, start_id_to_intv:, lape_lang[layer]] = replace_value_nt\n                    else:\n                        output[0, start_id_to_intv:, lape_lang[layer]] += replace_value_nt\n                    \n        # elif replace_method == \"all\":\n        #     lape_target_lang = lape_langs[target_lang]\n        #     if is_multiplied:\n        #         output[0, start_id_to_intv:, (torch.arange(output.shape[-1]) != lape_target_lang[layer]).nonzero(as_tuple=True)[0]] *= replace_value\n        #     else: \n        #         output[0, start_id_to_intv:, (torch.arange(output.shape[-1]) != lape_target_lang[layer]).nonzero(as_tuple=True)[0]]  = replace_value\n                \n    return hook_fn\n\ndef intervensi(prompt, infer_model, lape_lang, num_layers, max_new_tokens, operation):\n    clean_hooks(infer_model)\n    handlers = []\n    for i in range (num_layers):\n        mlp = infer_model.model.model.layers[i].mlp\n        handlers.append(mlp.act_fn.register_forward_hook(set_activation_mlp(infer_model.model_name, f\"{i}\", lape_lang, operation)))\n    test_inference(infer_model, prompt, max_new_tokens)\n    for i in handlers:\n        i.remove()\n    clean_hooks(infer_model)\n\ndef intervensi_w_target_lang(replace_method, prompt, infer_model, lape_langs, target_lang, num_layers, max_new_tokens, operation_non_target, operation_target, range_layers):\n    \n    clean_hooks(infer_model)\n    handlers = []\n    assert range_layers[-1] <= num_layers\n    for i in (range_layers):\n        mlp = infer_model.model.model.layers[i].mlp\n        handlers.append(mlp.act_fn.register_forward_hook(set_activation_mlp_v2(replace_method, infer_model.model_name, f\"{i}\", lape_langs, target_lang, operation_non_target, operation_target)))\n    test_inference(infer_model, prompt, max_new_tokens)\n    for i in handlers:\n        i.remove()\n    clean_hooks(infer_model)\n    \ndef intervene_langs_w_target(\n    replace_method, prompt, infer_model, lape_langs, num_layers, \n    target_lang=None, max_new_tokens=30, operation_non_target=\"=0\", operation_target=\"*1\", range_layers=range(1)):\n    \"\"\"\n    target_lang: default None, kalau ada berarti cuman lihat di bahasa target_lang itu aja else semuanya \n    \"\"\"\n    print(f\"🐸original: \")\n    test_inference(infer_model, prompt, max_new_tokens)\n    if target_lang:\n        print(f\"🐸 intervened w.r.t. a target language {languages[target_lang]}: \")\n        intervensi_w_target_lang(replace_method, prompt, infer_model, lape_langs, target_lang, num_layers, max_new_tokens, operation_non_target, operation_target, range_layers)\n    else:\n        for key, value in lape_langs.items():\n            print(f\"🐸 intervened w.r.t. a target language {languages[key]}: \")\n            intervensi_w_target_lang(replace_method, prompt, infer_model, lape_langs, key, num_layers, max_new_tokens, operation_non_target, operation_target, range_layers)\n\n    clean_hooks(infer_model)\ndef intervene_langs(infer_model, prompt_lang, lape_langs, num_layers, max_new_tokens=10, operation=\"=0\"):\n    \"\"\"\n    given a prompt, from number of languages, see how intervention each language affects the prompt.\n    infer_model: InferenceModel\n    prompt_lang: (str) prompt dalam bahasa tertentu\n    lape_langs: (dict) key: lang, value: lape in a language\n    \"\"\"\n    print(f\"🐸original: \")\n    test_inference(infer_model, prompt_lang, max_new_tokens)\n    for key, value in lape_langs.items():\n        print(f\"🐸intervensi {languages[key]}\")\n        intervensi(prompt_lang, infer_model, value, num_layers, max_new_tokens, operation)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Notes:\n\n- di TransformerLens, `model.to_tokens(text)` sama dengan `model.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].to(device)` ini yg dimasukin ke model(). text harus berupa hasil tokenizer pake apply_chat_template (`text = model.tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)`). ","metadata":{}},{"cell_type":"code","source":"# import torch\n\n# def get_token_sequence_rank(model, messages, target_text, device=\"cuda\"):\n#     \"\"\"\n#     Get the rank of a multi-token sequence in the model's output probabilities.\n\n#     Args:\n#         model: The Hugging Face model (Qwen or similar).\n#         messages (list): The chat messages in OpenAI format.\n#         target_text (str): The full text whose token sequence rank we want to find.\n#         device (str): The device to run the model on (\"cuda\" or \"cpu\").\n\n#     Returns:\n#         avg_rank (float): The average rank of the tokens (lower is better).\n#         avg_probability (float): The geometric mean of token probabilities.\n#         token_ranks (list): The individual ranks of each token in the sequence.\n#         token_probs (list): The probability of each token.\n#     \"\"\"\n#     # Convert messages to full text\n#     text = model.tokenizer.apply_chat_template(\n#         messages,\n#         tokenize=False,\n#         add_generation_prompt=True\n#     )\n\n#     # Tokenize input and target text\n#     filled_input_ids = model.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n#     target_token_ids = model.tokenizer(target_text, add_special_tokens=False)[\"input_ids\"]\n#     print(target_token_ids)\n#     # Get logits from the model\n#     logits = model.model(filled_input_ids, remove_batch_dim=True).logits\n\n#     token_ranks = []\n#     token_probs = []\n\n#     for i, token_id in enumerate(target_token_ids):\n#         # Compute softmax probabilities for token position `-len(target_token_ids) + i`\n#         probs = torch.softmax(logits[:, -(len(target_token_ids) - i), :], dim=-1)\n\n#         # Sort token probabilities in descending order\n#         sorted_indices = torch.argsort(probs, descending=True)\n\n#         # Find the rank of the token\n#         target_rank = (sorted_indices == token_id).nonzero(as_tuple=True)[1]\n\n#         if target_rank.numel() == 0:\n#             rank = -1  # Token not found\n#             probability = 0.0\n#         else:\n#             rank = target_rank.item() + 1  # Convert to 1-based rank\n#             probability = probs[0, token_id].item()\n\n#         token_ranks.append(rank)\n#         token_probs.append(probability)\n\n#     # Compute aggregate ranking and probability\n#     avg_rank = sum(token_ranks) / len(token_ranks)\n#     avg_probability = torch.prod(torch.tensor(token_probs)).item()  # Geometric mean\n\n#     return avg_rank, avg_probability, token_ranks, token_probs\n    \n# prompts = [\"What is the capital city of England?\"]\n\n# messages = [\n#     {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n#     {\"role\": \"user\", \"content\": prompts[0]}\n# ]\n\n# target_text = \"The \"  # Can be a multi-token word\n\n# avg_rank, avg_prob, ranks, probs = get_token_sequence_rank(model, messages, target_text)\n\n# print(f\"Average Rank: {avg_rank}\")\n# print(f\"Geometric Mean Probability: {avg_prob:.4f}\")\n# print(f\"Individual Token Ranks: {ranks}\")\n# print(f\"Individual Token Probabilities: {probs}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rich import print as rprint\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\ndef test_prompt(\n    prompt_tokens: torch.tensor , # (tokenized prompts)\n    answer: Union[str, list[str]],\n    model,  # Can't give type hint due to circular imports\n    prepend_space_to_answer: bool = True,\n    print_details: bool = True,\n    prepend_bos: Optional[bool] = True,\n    top_k: int = 10,\n) -> None:\n    \"\"\"Test if the Model Can Give the Correct Answer to a Prompt.\n\n    Intended for exploratory analysis. Prints out the performance on the answer (rank, logit, prob),\n    as well as the top k tokens. Works for multi-token prompts and multi-token answers.\n\n    Warning:\n\n    This will print the results (it does not return them).\n\n    Examples:\n\n    >>> from transformer_lens import HookedTransformer, utils\n    >>> model = HookedTransformer.from_pretrained(\"tiny-stories-1M\")\n    Loaded pretrained model tiny-stories-1M into HookedTransformer\n\n    >>> prompt = \"Why did the elephant cross the\"\n    >>> answer = \"road\"\n    >>> utils.test_prompt(prompt, answer, model)\n    Tokenized prompt: ['<|endoftext|>', 'Why', ' did', ' the', ' elephant', ' cross', ' the']\n    Tokenized answer: [' road']\n    Performance on answer token:\n    Rank: 2        Logit: 14.24 Prob:  3.51% Token: | road|\n    Top 0th token. Logit: 14.51 Prob:  4.59% Token: | ground|\n    Top 1th token. Logit: 14.41 Prob:  4.18% Token: | tree|\n    Top 2th token. Logit: 14.24 Prob:  3.51% Token: | road|\n    Top 3th token. Logit: 14.22 Prob:  3.45% Token: | car|\n    Top 4th token. Logit: 13.92 Prob:  2.55% Token: | river|\n    Top 5th token. Logit: 13.79 Prob:  2.25% Token: | street|\n    Top 6th token. Logit: 13.77 Prob:  2.21% Token: | k|\n    Top 7th token. Logit: 13.75 Prob:  2.16% Token: | hill|\n    Top 8th token. Logit: 13.64 Prob:  1.92% Token: | swing|\n    Top 9th token. Logit: 13.46 Prob:  1.61% Token: | park|\n    Ranks of the answer tokens: [(' road', 2)]\n\n    Args:\n        prompt:\n            The prompt string, e.g. \"Why did the elephant cross the\".\n        answer:\n            The answer, e.g. \"road\". Note that if you set prepend_space_to_answer to False, you need\n            to think about if you have a space before the answer here (as e.g. in this example the\n            answer may really be \" road\" if the prompt ends without a trailing space). If this is a\n            list of strings, then we only look at the next-token completion, and we compare them all\n            as possible model answers.\n        model:\n            The model.\n        prepend_space_to_answer:\n            Whether or not to prepend a space to the answer. Note this will only ever prepend a\n            space if the answer doesn't already start with one.\n        print_details:\n            Print the prompt (as a string but broken up by token), answer and top k tokens (all\n            with logit, rank and probability).\n        prepend_bos:\n            Overrides self.cfg.default_prepend_bos if set. Whether to prepend\n            the BOS token to the input (applicable when input is a string). Models generally learn\n            to use the BOS token as a resting place for attention heads (i.e. a way for them to be\n            \"turned off\"). This therefore often improves performance slightly.\n        top_k:\n            Top k tokens to print details of (when print_details is set to True).\n\n    Returns:\n        None (just prints the results directly).\n    \"\"\"\n    tokenizer = model.tokenizer\n    model = model.model\n    answers = [answer] if isinstance(answer, str) else answer\n    n_answers = len(answers)\n    using_multiple_answers = n_answers > 1\n\n    if prepend_space_to_answer:\n        answers = [answer if answer.startswith(\" \") else \" \" + answer for answer in answers]\n\n    # GPT-2 often treats the first token weirdly, so lets give it a resting position\n    # prompt_tokens = model.to_tokens(prompt, prepend_bos=prepend_bos)\n\n    # prompt_tokens = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n    # Prepend BOS token if available\n    if prepend_bos and tokenizer.bos_token_id is not None:\n        prompt_tokens = [tokenizer.bos_token_id] + prompt_tokens\n\n    \n    # answer_tokens = model.to_tokens(answers, prepend_bos=False)\n\n\n    answer_tokens = tokenizer(answers, add_special_tokens=False)[\"input_ids\"]\n    # Prepend BOS token if available\n    if prepend_bos and tokenizer.bos_token_id is not None:\n        answer_tokens = [tokenizer.bos_token_id] + answer_tokens\n    answer_tokens = torch.tensor(answer_tokens)\n    prompt_tokens = torch.tensor(prompt_tokens)\n    # If we have multiple answers, we're only allowed a single token generation\n    if using_multiple_answers:\n        answer_tokens = answer_tokens[:, :1]\n\n    # Deal with case where answers is a list of strings\n    prompt_tokens = prompt_tokens.repeat(answer_tokens.shape[0], 1)\n    tokens = torch.cat((prompt_tokens, answer_tokens), dim=1)\n    # temp_tokens = tokenizer(prompt_tokens, add_special_tokens=False)[\"input_ids\"]\n    # Prepend BOS token if available\n    if prepend_bos and tokenizer.bos_token_id is not None:\n        prompt_tokens = [tokenizer.bos_token_id] + prompt_tokens\n    \n    # Convert token IDs to a list of string tokens\n    prompt_str_tokens = tokenizer.convert_ids_to_tokens(prompt_tokens[0])\n    # prompt_str_tokens = model.to_str_tokens(prompt, prepend_bos=prepend_bos)\n\n\n    temp_tokens = [tokenizer(answer, add_special_tokens=False)[\"input_ids\"] for answer in answers]\n    answer_str_tokens_list = [tokenizer.convert_ids_to_tokens(temp) for temp in temp_tokens]\n    prompt_length = len(prompt_str_tokens)\n    answer_length = 1 if using_multiple_answers else len(answer_str_tokens_list[0])\n    if print_details:\n        print(\"Tokenized prompt:\", prompt_str_tokens)\n        if using_multiple_answers:\n            print(\"Tokenized answers:\", answer_str_tokens_list)\n        else:\n            print(\"Tokenized answer:\", answer_str_tokens_list[0])\n    tokens = tokens.to(device)\n    logits = model(tokens).logits\n    probs = logits.softmax(dim=-1)\n    answer_ranks = []\n\n    for index in range(prompt_length, prompt_length + answer_length):\n        # Get answer tokens for this sequence position\n        answer_tokens = tokens[:, index]\n        answer_str_tokens = [a[index - prompt_length] for a in answer_str_tokens_list]\n        # Offset by 1 because models predict the NEXT token\n        token_probs = probs[:, index - 1]\n        sorted_token_probs, sorted_token_positions = token_probs.sort(descending=True)\n        answer_token_ranks = sorted_token_positions.argsort(-1)[\n            range(n_answers), answer_tokens.cpu()\n        ].tolist()\n        answer_ranks.append(\n            [\n                (answer_str_token, answer_token_rank)\n                for answer_str_token, answer_token_rank in zip(\n                    answer_str_tokens, answer_token_ranks\n                )\n            ]\n        )\n        if print_details:\n            # String formatting syntax - the first number gives the number of characters to pad to, the second number gives the number of decimal places.\n            # rprint gives rich text printing\n            rprint(\n                f\"Performance on answer token{'s' if n_answers > 1 else ''}:\\n\"\n                + \"\\n\".join(\n                    [\n                        f\"[b]Rank: {answer_token_ranks[i]: <8} Logit: {logits[i, index-1, answer_tokens[i]].item():5.2f} Prob: {token_probs[i, answer_tokens[i]].item():6.2%} Token: |{answer_str_tokens[i]}|[/b]\"\n                        for i in range(n_answers)\n                    ]\n                )\n            )\n            for i in range(top_k):\n                print(\n                    f\"Top {i}th token. Logit: {logits[0, index-1, sorted_token_positions[0, i]].item():5.2f} Prob: {sorted_token_probs[0, i].item():6.2%} Token: |{tokenizer.decode(sorted_token_positions[0, i])}|\"\n                )\n\n    # If n_answers = 1 then unwrap answer ranks, so printed output matches original version of function\n    if not using_multiple_answers:\n        single_answer_ranks = [r[0] for r in answer_ranks]\n        rprint(f\"[b]Ranks of the answer tokens:[/b] {single_answer_ranks}\")\n        return single_answer_ranks\n    else:\n        rprint(f\"[b]Ranks of the answer tokens:[/b] {answer_ranks}\")\n        return answer_ranks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prompts = [\"What is the capital city of England?\"]\n\n# messages = [\n#             {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n#             {\"role\": \"user\", \"content\": prompts[0]}\n#         ]\n\n# # model.tokenizer.decode(sorted_token_positions[0, i])\n# text = model.tokenizer.apply_chat_template(\n#     messages,\n#     tokenize=False,\n#     add_generation_prompt=True\n# )\n# prompt = model.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n# test_prompt(prompt, \" The capital city of England is London\", model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ranks = test_prompt(prompt, \" The capital city of England is London\", model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x = ranks.copy()\n# x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"markdown","source":"TODO BRO","metadata":{}},{"cell_type":"code","source":"# from datasets import load_dataset\n\n# ds = load_dataset(\"coastalcph/mpararel\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data = (ds['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# languages = {'en', 'it', 'fr', 'sp', 'zh'}  # Set of desired languages\n\n# ds_filtered30 = ds['train'].filter(lambda example: example['language'] in languages and \n#                                                   example['relation'] == 'P30' and \n#                                                   example['template_id'] == 0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for i in ds_filtered30.select(range(30)):\n#     print(i)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for i in range (3):\n#     print(ds[i])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluate","metadata":{}},{"cell_type":"code","source":"model.model.config.num_hidden_layers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom functools import reduce\n\n\n\ndef calc_logprob_answer(text, answer, model):\n    answer = f\" {answer}\" if not answer.startswith(\" \") else answer\n    filled_sentence = text + answer\n    filled_input_ids = model.tokenizer(filled_sentence, return_tensors=\"pt\")[\"input_ids\"].to(device)\n    str_tokens = model.tokenizer.convert_ids_to_tokens(filled_input_ids[0])\n    # print(f\"str_tokens text: {str_tokens}\")\n    with torch.no_grad():\n        outputs = model.model(input_ids=filled_input_ids)\n        logits = outputs.logits  # Shape: (batch_size, seq_len, vocab_size)\n    log_probs = F.log_softmax(logits, dim=-1)\n    seq_log_probs = 0\n    for i in range(1, filled_input_ids.size(1)):  \n        token_id = filled_input_ids[0, i]\n        seq_log_probs += log_probs[0, i-1, token_id].item()\n    # print(f\"seq_log_probs: {seq_log_probs}\")\n    return seq_log_probs\n\ndef calculate_logprob_diff(model, is_intervening, intervention_data, lang_target, lang_baseline, lape_langs, operation_target, range_layers, operation_non_target=\"*1\"):\n    \"\"\"\n    calculate the log prob diff of a language intervention scenario.\n    For example: for a baseline en and target it, calculate the logprob(answer_it) - logprob(answer_en)\n    for each prompt\n\n    Output: \n    [(question_id, logprob_diff_answer_target_baseline)]\n    e.g.\n    (question_id, logprob(\" Londra\") - logprob(\" London\"))\n    \"\"\"\n    \n    logprob_diff_lang = []\n    tokenizer = model.tokenizer\n\n    #TODO REGISTER HOOK ON LANG TARGET ONLY IF LANG TARGET != LANG BASELINE\n    num_layers = model.model.config.num_hidden_layers\n    \n    if model.model_name == 'Qwen/Qwen2.5-0.5B-Instruct' or model.model_name == 'Qwen/Qwen2.5-3B-Instruct':\n        for data in intervention_data:\n            prompt = data['query'][lang_baseline]\n            \n            messages = [\n                {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n            text = tokenizer.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True\n            )\n            answer_target = data['answers'][lang_target]\n            answer_baseline = data['answers'][lang_baseline]\n\n\n            logprob_answer_target = 0\n            logprob_answer_baseline = 0\n            if is_intervening:\n                clean_hooks(model)\n                handlers = []\n                assert range_layers[-1] <= num_layers\n                for i in (range_layers):\n                    mlp = model.model.model.layers[i].mlp\n                    handlers.append(mlp.act_fn.register_forward_hook(\n                        set_activation_mlp_v2(replace_method=\"lape\", model_name=model.model_name, name=f\"{i}\",\n                          lape_langs=lape_langs, target_lang=lang_target, operation_non_target=operation_non_target, \n                          operation_target=operation_target)))\n                logprob_answer_target = calc_logprob_answer(text, answer_target, model)\n                logprob_answer_baseline= calc_logprob_answer(text, answer_baseline, model)\n                for i in handlers:\n                    i.remove()\n                clean_hooks(model)\n            else: \n                logprob_answer_target = calc_logprob_answer(text, answer_target, model)\n                logprob_answer_baseline= calc_logprob_answer(text, answer_baseline, model)\n            log_prob_diff = logprob_answer_target-logprob_answer_baseline\n            question_id = data['template_id']\n            logprob_diff_lang.append((question_id, log_prob_diff))\n    else:\n        print(\"MODEL NOT AVAILABLE!\")\n    return logprob_diff_lang\n\ndef calculate_diff_and_ratio(logprob_diff_baseline, logprob_diff_target):\n    baseline_dict = dict(logprob_diff_baseline)\n    target_dict = dict(logprob_diff_target)\n    diff_of_diff = [\n        (qid, target_dict[qid] - baseline_dict[qid]) \n        for qid in baseline_dict.keys() & target_dict.keys()\n    ]\n    ratio_of_diff = [\n        (qid, abs(target_dict[qid] / baseline_dict[qid]) if baseline_dict[qid] != 0 else float('inf')) \n        for qid in baseline_dict.keys() & target_dict.keys()\n    ]\n    return diff_of_diff, ratio_of_diff\ndef eval_log_probs(model, intervention_data, lang_baseline, lang_target, lape_langs, operation_target, range_layers, return_df=False):\n    \"\"\"\n    INPUT:\n        model: HookTransformer model\n        intervention_data : list of dictionary, e.g.\n            [{\n            'template_id' : 'question_id',\n            'query': 'question', \n            'answers' : \n                {'en' : 'answer_en', \n                'id' : 'answer_id'\n                \n                }}]\n        lang_baseline: str lang baseline e.g. 'en'\n        lang_target: str lang target e.g. 'it'\n        \n    OUTPUT: beururtan sama template_id\n        log_prob_diff = \n        {\n            'lang_baseline': [(question_id, log_prob_diff_answer_target_baseline_q1), (question_id, log_prob_diff_answer_target_baseline_q2),... ],\n            'lang_target': [(question_id, log_prob_diff_answer_target_baseline_q1), (question_id, log_prob_diff_answer_target_baseline_q2),... ],\n        },\n        df (Optional)\n    \"\"\"\n    logprob_diff_baseline = calculate_logprob_diff(model, False, intervention_data, lang_target, lang_baseline, lape_langs, operation_target, range_layers)\n    logprob_diff_target = calculate_logprob_diff(model, True, intervention_data, lang_target, lang_baseline, lape_langs, operation_target, range_layers)\n    if return_df:\n        df_logprob_diff_baseline = pd.DataFrame(logprob_diff_baseline, columns=[\"qid\", \"logprob_baseline_diff\"])\n        df_logprob_diff_target = pd.DataFrame(logprob_diff_target, columns=[\"qid\", \"logprob_diff_target\"])\n        data = []\n        for item in intervention_data:\n            qid = item[\"template_id\"]\n            question = item['query'].get(lang_baseline, None)\n            answer_baseline = item[\"answers\"].get(lang_baseline, None)  # English (en-US)\n            answer_target = item[\"answers\"].get(lang_target, None)   # French (fr-FR)\n            data.append((qid, question, answer_baseline, answer_target))\n        df_answers = pd.DataFrame(data, columns=[\"qid\", f\"question {languages[lang_baseline]}\", f\"answer_baseline {languages[lang_baseline]}\", f\"answer_target {languages[lang_target]}\"])\n        diff_of_diff, ratio_of_diff = calculate_diff_and_ratio(logprob_diff_baseline, logprob_diff_target)\n        # print(diff_of_diff)\n        # print(f\"situ: {logprob_diff_baseline}\")\n        # print(f\"situ: {diff_of_diff}\")\n\n        df_diff_of_diff = pd.DataFrame(diff_of_diff, columns=[\"qid\", \"diff_of_diff\"])\n        df_ratio_of_diff = pd.DataFrame(ratio_of_diff, columns=[\"qid\", \"ratio_of_diff\"])\n        dfs = [df_answers, df_logprob_diff_baseline, df_logprob_diff_target, df_diff_of_diff, df_ratio_of_diff]\n        \n        df_final = reduce(lambda left, right: pd.merge(left, right, on=\"qid\", how=\"outer\"), dfs)\n        # print(df_final)\n        return logprob_diff_target, logprob_diff_baseline, df_final\n        \n    return logprob_diff_target, logprob_diff_baseline\n\ndef beautify_df(df, lang_target, lang_baseline):\n    df_display = df.rename(columns={\n    \"qid\": f\"qid in {lang_baseline}\",\n    \"logprob_baseline_diff\": f\"logprob {lang_target} answer - {lang_baseline} answer in baseline\",\n    \"logprob_diff_target\": f\"logprob {lang_target} answer - {lang_baseline} answer in {lang_target} intervention\",\n    \"diff_of_diff\": f\"Difference of Differences {lang_target} - {lang_baseline}\",\n    \"ratio_of_diff\": f\"Ratio of Differences {lang_target} - {lang_baseline}\",\n    \"answer_baseline\": f\"Baseline Answer in {lang_baseline}\",\n    \"answer_target\": f\"Target Answer in {lang_target}\"\n})\n    return df_display\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from collections import defaultdict\n# import random\n\n# # Define the languages you want (assuming these are the ones in the dataset)\n# languages = {'en', 'it', 'fr', 'es', 'zh'}  # Using 'es' for Spanish instead of 'sp'\n\n# # Filter dataset to include only relations P30 and P36\n# ds_filtered = ds['train'].filter(lambda example: example['language'] in languages and \n#                                                  example['relation'] in {'P30', 'P36'} and \n#                                                  example['template_id'] == 0)\n\n# # Group by template_id\n# grouped_by_template = defaultdict(list)\n# for example in ds_filtered:\n#     grouped_by_template[example['template_id']].append(example)\n\n# # Ensure unique template IDs and 5 languages per question\n# intervention_data = []\n# used_qids = set()\n\n# # Shuffle the template IDs for randomness\n# template_ids = list(grouped_by_template.keys())\n# random.shuffle(template_ids)\n\n# for template_id in template_ids:\n#     examples = grouped_by_template[template_id]\n    \n#     # Ensure we have enough languages (at least 5)\n#     language_map = {ex['language']: ex for ex in examples if ex['language'] in languages}\n    \n#     if len(language_map) == 5:\n#         qid = examples[0]['id']  # Get a unique QID from one of the examples\n#         if qid not in used_qids:\n#             used_qids.add(qid)\n            \n#             # Construct query-answer pairs\n#             query = {lang_code: ex['question'] for lang_code, ex in language_map.items()}\n#             answer = {lang_code: ex['answer'] for lang_code, ex in language_map.items()}\n\n#             intervention_data.append({\n#                 \"template_id\": template_id,\n#                 \"query\": query,\n#                 \"answers\": answer\n#             })\n    \n#     if len(intervention_data) >= 50:\n#         break  # Stop once we have 50 unique questions\n\n# # Print a sample to verify\n# print(intervention_data[:3])  # Show first 3 samples for validation\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"intervention_data = [\n    {\n        \"template_id\": 1,\n        \"query\": {\n            10: \"What is the hardest natural substance on Earth?\",\n            21: \"Qual è la sostanza naturale più dura sulla Terra?\",\n            19: \"Apa zat alami terkeras di Bumi?\",\n            14: \"Quelle est la substance naturelle la plus dure sur Terre?\",\n            11: \"¿Cuál es la sustancia natural más dura de la Tierra?\"\n        },\n        \"answers\": {\n            10: \"Diamond\",\n            21: \"Diamante\",\n            19: \"Berlian\",\n            14: \"Diamant\",\n            11: \"Diamante\"\n        }\n    },\n    {\n        \"template_id\": 2,\n        \"query\": {\n            10: \"What is the fastest land animal?\",\n            21: \"Qual è l'animale terrestre più veloce?\",\n            19: \"Apa hewan darat tercepat?\",\n            14: \"Quel est l'animal terrestre le plus rapide?\",\n            11: \"¿Cuál es el animal terrestre más rápido?\"\n        },\n        \"answers\": {\n            10: \"Cheetah\",\n            21: \"Ghepardo\",\n            19: \"Cheetah\",\n            14: \"Guépard\",\n            11: \"Guepardo\"\n        }\n    },\n    {\n        \"template_id\": 3,\n        \"query\": {\n            10: \"What is the first element on the periodic table?\",\n            21: \"Qual è il primo elemento della tavola periodica?\",\n            19: \"Apa unsur pertama dalam tabel periodik?\",\n            14: \"Quel est le premier élément du tableau périodique?\",\n            11: \"¿Cuál es el primer elemento de la tabla periódica?\"\n        },\n        \"answers\": {\n            10: \"Hydrogen\",\n            21: \"Idrogeno\",\n            19: \"Hidrogen\",\n            14: \"Hydrogène\",\n            11: \"Hidrógeno\"\n        }\n    },\n    {\n        \"template_id\": 4,\n        \"query\": {\n            10: \"How many continents are there on Earth?\",\n            21: \"Quanti continenti ci sono sulla Terra?\",\n            19: \"Ada berapa benua di Bumi?\",\n            14: \"Combien y a-t-il de continents sur Terre?\",\n            11: \"¿Cuántos continentes hay en la Tierra?\"\n        },\n        \"answers\": {\n            10: \"Seven\",\n            21: \"Sette\",\n            19: \"Tujuh\",\n            14: \"Sept\",\n            11: \"Siete\"\n        }\n    },\n    {\n        \"template_id\": 5,\n        \"query\": {\n            10: \"What is the powerhouse of the cell?\",\n            21: \"Qual è la centrale energetica della cellula?\",\n            19: \"Apa pusat tenaga dalam sel?\",\n            14: \"Quelle est la centrale énergétique de la cellule?\",\n            11: \"¿Cuál es la central energética de la célula?\"\n        },\n        \"answers\": {\n            10: \"Mitochondria\",\n            21: \"Mitocondrio\",\n            19: \"Mitokondria\",\n            14: \"Mitochondrie\",\n            11: \"Mitocondria\"\n        }\n    },\n    {\n        \"template_id\": 6,\n        \"query\": {\n            10: \"Which organelle is responsible for photosynthesis?\",\n            21: \"Quale organello è responsabile della fotosintesi?\",\n            19: \"Organel mana yang bertanggung jawab atas fotosintesis?\",\n            14: \"Quel organite est responsable de la photosynthèse?\",\n            11: \"¿Qué orgánulo es responsable de la fotosíntesis?\"\n        },\n        \"answers\": {\n            10: \"Chloroplast\",\n            21: \"Cloroplasto\",\n            19: \"Kloroplas\",\n            14: \"Chloroplaste\",\n            11: \"Cloroplasto\"\n        }\n    },\n    {\n        \"template_id\": 7,\n        \"query\": {\n            10: \"What is the basic unit of life?\",\n            21: \"Qual è l'unità di base della vita?\",\n            19: \"Apa unit dasar kehidupan?\",\n            14: \"Quelle est l'unité de base de la vie?\",\n            11: \"¿Cuál es la unidad básica de la vida?\"\n        },\n        \"answers\": {\n            10: \"Cell\",\n            21: \"Cellula\",\n            19: \"Sel\",\n            14: \"Cellule\",\n            11: \"Célula\"\n        }\n    },\n    {\n        \"template_id\": 8,\n        \"query\": {\n            10: \"What is the biggest animal on land?\",\n            21: \"Qual è il più grande animale terrestre?\",\n            19: \"Apa hewan terbesar di darat?\",\n            14: \"Quel est le plus grand animal terrestre?\",\n            11: \"¿Cuál es el animal más grande en tierra?\"\n        },\n        \"answers\": {\n            10: \"African elephant\",\n            21: \"Elefante africano\",\n            19: \"Gajah Afrika\",\n            14: \"Éléphant d'Afrique\",\n            11: \"Elefante africano\"\n        }\n    },\n    {\n        \"template_id\": 9,\n        \"query\": {\n            10: \"What is the coldest continent in the world?\",\n            21: \"Qual è il continente più freddo del mondo?\",\n            19: \"Apa benua terdingin di dunia?\",\n            14: \"Quel est le continent le plus froid du monde?\",\n            11: \"¿Cuál es el continente más frío del mundo?\"\n        },\n        \"answers\": {\n            10: \"Antarctica\",\n            21: \"Antartide\",\n            19: \"Antartika\",\n            14: \"Antarctique\",\n            11: \"Antártida\"\n        }\n    },\n    {\n        \"template_id\": 10,\n        \"query\": {\n            10: \"What season does Australia experience in December?\",\n            21: \"Quale stagione vive l'Australia a dicembre?\",\n            19: \"Musim apa yang dialami Australia pada bulan Desember?\",\n            14: \"Quelle saison l'Australie vit-elle en décembre?\",\n            11: \"¿Qué estación experimenta Australia en diciembre?\"\n        },\n        \"answers\": {\n            10: \"Summer\",\n            21: \"Estate\",\n            19: \"Musim panas\",\n            14: \"Été\",\n            11: \"Verano\"\n        }\n    },\n    {\n        \"template_id\": 11,\n        \"query\": {\n            10: \"What is the proper geographical term for a cluster or chain of islands?\",\n            21: \"Qual è il termine geografico corretto per un gruppo o una catena di isole?\",\n            19: \"Apa istilah geografis yang benar untuk sekelompok atau rantai pulau?\",\n            14: \"Quel est le terme géographique approprié pour un groupe ou une chaîne d'îles?\",\n            11: \"¿Cuál es el término geográfico adecuado para un grupo o cadena de islas?\"\n        },\n        \"answers\": {\n            10: \"Archipelago\",\n            21: \"Arcipelago\",\n            19: \"Kepulauan\",\n            14: \"Archipel\",\n            11: \"Archipiélago\"\n        }\n    }\n    \n]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"languages_reversed = {v: k for k, v in languages.items()}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LAPE 2%","metadata":{}},{"cell_type":"code","source":"lape_massive_qwen_2 = visualize_lape(full_languages_raw_values, 24, top_rate = 0.02,filter_rate=0.80,activation_bar_ratio=0.80)\nlape_langs = {key:lape_massive_qwen_2[key] for key, value in enumerate(lape_massive_qwen_2)}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Difference of Difference (DoD) > 0 → The first difference is larger than the second.\n## |Ratio of Difference (RoD)| > 1 → The first difference is proportionally larger than the second.","metadata":{}},{"cell_type":"code","source":"from IPython.display import display\n\n\ndef eval_intervention_a_baselang(intervention_data, lang_baseline, lang_targets, operation_target, range_layers, lape_langs, return_df=True, save_each_df = False):\n    diff_dict = {key:None for key in lang_targets}\n    for lang_target in lang_targets:\n        id_lang_baseline = languages_reversed[lang_baseline]\n        id_lang_target = languages_reversed[lang_target]\n        diff_target, diff_baseline, df = eval_log_probs(\n            model, intervention_data, \n            lang_baseline=id_lang_baseline, \n            lang_target=id_lang_target, lape_langs=lape_langs, \n            operation_target=operation_target, range_layers=range_layers, \n            return_df=return_df)\n        # count average diff of diff and ratio of diff\n        # print(f\"sini: {diff_baseline}\")\n        diff_of_diff, ratio_of_diff = calculate_diff_and_ratio(diff_baseline, diff_target)\n        # print(f\"sini: {diff_of_diff}\")\n        positive_count_dod = sum(1 for _, diff in diff_of_diff if diff > 0)\n        # print(positive_count_dod)\n        average_dod = positive_count_dod / len(diff_of_diff) if len(diff_of_diff) > 0 else 0\n        # print(average_dod)\n        positive_count_rod = sum(1 for _, diff in ratio_of_diff if diff != float('inf') and diff > 1)\n        average_rod = positive_count_rod / len(ratio_of_diff) if len(ratio_of_diff) > 0 else 0\n        # print(positive_count_rod)\n        # print(average_rod)\n        \n        diff_dict[lang_target] = (average_dod, average_rod)\n        df_display =beautify_df(df, lang_target, lang_baseline)\n        # df_display.head(20)\n        # print(df_display)\n        display(df_display)\n        if save_each_df:\n            df_display.to_csv(f\"baseline_{lang_baseline}/{lang_target}.csv\")\n    # print(diff_dict)\n    df_result = pd.DataFrame.from_dict(diff_dict, orient='index', columns=['Diff of diff percentage', 'Ratio of diff percentage']).reset_index()\n    # Rename the columns properly\n    df_result.rename(columns={'index': f'neuron intervention (baseline {lang_baseline})'}, inplace=True)\n    display(df_result)\n    return df_result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MODIFY  THIS ONLY \nlang_baseline = \"Italian (it-IT)\"\nlang_targets = ['French (fr-FR)', \"English (en-US)\", \"Indonesian (id-ID)\", \"Spanish (es-ES)\"]\noperation_target = \"+2\"\nrange_layers=[5,17,18,19,20,21,22,23]\nlape_langs = lape_langs\n#############\ndf_italy = eval_intervention_a_baselang(intervention_data, lang_baseline, lang_targets, operation_target, range_layers, lape_langs, return_df=True, save_each_df = False)\ndf_italy.to_csv(\"df_italy.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MODIFY  THIS ONLY \nlang_baseline = \"English (en-US)\"\nlang_targets = ['French (fr-FR)', \"Indonesian (id-ID)\", \"Spanish (es-ES)\", \"Italian (it-IT)\"]\noperation_target = \"+2\"\nrange_layers=[5,17,18,19,20,21,22,23]\nlape_langs = lape_langs\n#############\ndf_english = eval_intervention_a_baselang(intervention_data, lang_baseline, lang_targets, operation_target, range_layers, lape_langs, return_df=True, save_each_df = False)\n\ndf_english.to_csv(\"df_english.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MODIFY  THIS ONLY \nlang_baseline = \"Indonesian (id-ID)\" \nlang_targets = ['French (fr-FR)', \"English (en-US)\", \"Spanish (es-ES)\", \"Italian (it-IT)\"]\noperation_target = \"+2\"\nrange_layers=[5,17,18,19,20,21,22,23]\nlape_langs = lape_langs\n#############\ndf_indo = eval_intervention_a_baselang(intervention_data, lang_baseline, lang_targets, operation_target, range_layers, lape_langs, return_df=True, save_each_df = False)\ndf_indo.to_csv(\"df_indo.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MODIFY  THIS ONLY \nlang_baseline =  \"Spanish (es-ES)\"\nlang_targets = ['French (fr-FR)', \"English (en-US)\", \"Indonesian (id-ID)\", \"Italian (it-IT)\"]\noperation_target = \"+2\"\nrange_layers=[5,17,18,19,20,21,22,23]\nlape_langs = lape_langs\n#############\ndf_spanish = eval_intervention_a_baselang(intervention_data, lang_baseline, lang_targets, operation_target, range_layers, lape_langs, return_df=True, save_each_df = False)\ndf_spanish.to_csv(\"df_spanish.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MODIFY  THIS ONLY \nlang_baseline =  'French (fr-FR)'\nlang_targets = [\"Spanish (es-ES)\", \"English (en-US)\", \"Indonesian (id-ID)\", \"Italian (it-IT)\"]\noperation_target = \"+2\"\nrange_layers=[5,17,18,19,20,21,22,23]\nlape_langs = lape_langs\n#############\ndf_french = eval_intervention_a_baselang(intervention_data, lang_baseline, lang_targets, operation_target, range_layers, lape_langs, return_df=True, save_each_df = False)\ndf_french.to_csv(\"df_french.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Free text questions Eval","metadata":{}},{"cell_type":"markdown","source":"### prompt hanya bahasa inggris","metadata":{}},{"cell_type":"code","source":"languages_reversed","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"questions_answers = [\n    {\n    \"q\":\"My favourite food is\", \"answer_baseline\" : \"sandwich\", \n        \"answers_target\" : {\n            languages_reversed[\"French (fr-FR)\"] : \"croissant\", \n            languages_reversed[\"Italian (it-IT)\"]: \"pizza\"}\n    },\n    {\n    \"q\":\"My favourite football player is\", \"answer_baseline\" : \"David Beckham\", \n        \"answers_target\" : {\n            languages_reversed[\"French (fr-FR)\"] : \"Kylian Mbappe\", \n            languages_reversed[\"Portuguese (pt-PT)\"]: \"Cristiano Ronaldo\",\n            languages_reversed[\"Dutch (nl-NL)\"]: \"Virgil Van Dijk\"},\n    },\n    {\n    \"q\":\"My favourite city is\", \"answer_baseline\" : \"London\", \n        \"answers_target\" : {\n            languages_reversed[\"French (fr-FR)\"] : \"Paris\", \n            languages_reversed[\"Indonesian (id-ID)\"]: \"Jakarta\",\n            languages_reversed[\"Chinese (Simplified) (zh-CN)\"]: \"北京\"},\n    },\n    \n    \n                    \n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def free_text_eval(model, questions_answers, lape_langs, operation_target, range_layers, operation_non_target=\"*1\"):\n    rank_list = []\n    \n    num_layers = model.model.config.num_hidden_layers\n    for qna in questions_answers:\n        q = qna['q']\n        rank_langs = dict()\n        # messages = [\n        #         {\"role\": \"system\", \"content\": \"Continue the sentence in a natural way.\"},\n        #         {\"role\": \"user\", \"content\": q}\n        #     ]\n        text = q\n        # model.tokenizer.decode(sorted_token_positions[0, i])\n        # text = model.tokenizer.apply_chat_template(\n        #     messages,\n        #     tokenize=False,\n        #     add_generation_prompt=True\n        # )\n        \n        prompt = model.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n        print(model.tokenizer.convert_ids_to_tokens(prompt[0]))\n        print(\"\\n🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸 BASELINE 🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸\\n\")\n        \n        rank_baseline = test_prompt(prompt, qna[\"answer_baseline\"], model=model, top_k=20)\n        rank_langs[\"baseline\"] = rank_baseline\n        # print(rank_langs)\n        for lang_target,answer_target in qna['answers_target'].items():\n            \n            clean_hooks(model)\n            handlers = []\n            assert range_layers[-1] <= num_layers\n            for i in (range_layers):\n                mlp = model.model.model.layers[i].mlp\n                handlers.append(mlp.act_fn.register_forward_hook(\n                    set_activation_mlp_v2(replace_method=\"lape\", model_name=model.model_name, name=f\"{i}\",\n                      lape_langs=lape_langs, target_lang=lang_target, operation_non_target=operation_non_target, \n                      operation_target=operation_target)))\n            print(f\"\\n🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸 INTVD ON {languages[lang_target]} 🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸🐸\\n\")\n            \n            rank_target = test_prompt(prompt, answer_target,  model=model, top_k=20)\n            rank_langs[languages[lang_target]] = rank_target\n            for i in handlers:\n                i.remove()\n            clean_hooks(model)\n        # print(rank_langs)\n        rank_list.append(rank_langs)\n    return rank_list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MODIFY  THIS ONLY \noperation_target = \"+2\"\nrange_layers=[5,17,18,19,20,21,22,23]\n#############\nq0 = free_text_eval(model, questions_answers[:1], lape_langs, operation_target, range_layers, operation_non_target=\"*1\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MODIFY  THIS ONLY \noperation_target = \"+5\"\nrange_layers=range(24)\n#############\nq1 = free_text_eval(model, questions_answers[1:2], lape_langs, operation_target, range_layers, operation_non_target=\"*1\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MODIFY  THIS ONLY \noperation_target = \"*5\"\nrange_layers=range(24)\n#############\nq1 = free_text_eval(model, questions_answers[2:4], lape_langs, operation_target, range_layers, operation_non_target=\"*1\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LAPE 5%","metadata":{}},{"cell_type":"code","source":"lape_massive_qwen_5 = visualize_lape(full_languages_raw_values, 24, top_rate = 0.05,filter_rate=0.80,activation_bar_ratio=0.80)\nlape_langs_5 = {key:lape_massive_qwen_2[key] for key, value in enumerate(lape_massive_qwen_5)}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MODIFY  THIS ONLY \nimport os\nfolder_name = \"/kaggle/working/lape5\"\n\nif not os.path.exists(folder_name):\n    os.mkdir(folder_name)\nlang_baseline = \"Italian (it-IT)\"\nlang_targets = ['French (fr-FR)', \"English (en-US)\", \"Indonesian (id-ID)\", \"Spanish (es-ES)\"]\noperation_target = \"+2\"\nrange_layers=[5,17,18,19,20,21,22,23]\nlape_langs = lape_langs_5\n#############\ndf_italy = eval_intervention_a_baselang(intervention_data, lang_baseline, lang_targets, operation_target, range_layers, lape_langs, return_df=True, save_each_df = False)\ndf_italy.to_csv(\"lape5/5_df_italy.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MODIFY  THIS ONLY \nlang_baseline = \"English (en-US)\"\nlang_targets = ['French (fr-FR)', \"Indonesian (id-ID)\", \"Spanish (es-ES)\", \"Italian (it-IT)\"]\noperation_target = \"+2\"\nrange_layers=[5,17,18,19,20,21,22,23]\nlape_langs = lape_langs_5\n#############\ndf_english = eval_intervention_a_baselang(intervention_data, lang_baseline, lang_targets, operation_target, range_layers, lape_langs, return_df=True, save_each_df = False)\n\ndf_english.to_csv(\"lape5/5_df_english.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MODIFY  THIS ONLY \nlang_baseline = \"Indonesian (id-ID)\" \nlang_targets = ['French (fr-FR)', \"English (en-US)\", \"Spanish (es-ES)\", \"Italian (it-IT)\"]\noperation_target = \"+2\"\nrange_layers=[5,17,18,19,20,21,22,23]\nlape_langs = lape_langs_5\n#############\ndf_indo = eval_intervention_a_baselang(intervention_data, lang_baseline, lang_targets, operation_target, range_layers, lape_langs, return_df=True, save_each_df = False)\ndf_indo.to_csv(\"lape5/5_df_indo.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#MODIFY  THIS ONLY \nlang_baseline =  \"Spanish (es-ES)\"\nlang_targets = ['French (fr-FR)', \"English (en-US)\", \"Indonesian (id-ID)\", \"Italian (it-IT)\"]\noperation_target = \"+2\"\nrange_layers=[5,17,18,19,20,21,22,23]\nlape_langs = lape_langs_5\n#############\ndf_spanish = eval_intervention_a_baselang(intervention_data, lang_baseline, lang_targets, operation_target, range_layers, lape_langs, return_df=True, save_each_df = False)\ndf_spanish.to_csv(\"lape5/5_df_spanish.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#MODIFY  THIS ONLY \nlang_baseline =  'French (fr-FR)'\nlang_targets = [\"Spanish (es-ES)\", \"English (en-US)\", \"Indonesian (id-ID)\", \"Italian (it-IT)\"]\noperation_target = \"+2\"\nrange_layers=[5,17,18,19,20,21,22,23]\nlape_langs = lape_langs_5\n#############\ndf_french = eval_intervention_a_baselang(intervention_data, lang_baseline, lang_targets, operation_target, range_layers, lape_langs, return_df=True, save_each_df = False)\ndf_french.to_csv(\"lape5/5_df_french.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"free text","metadata":{}},{"cell_type":"code","source":"lape_langs = lape_langs_5\n\n#MODIFY  THIS ONLY \noperation_target = \"+2\"\nrange_layers=[5,17,18,19,20,21,22,23]\n#############\nq0 = free_text_eval(model, questions_answers[:1], lape_langs, operation_target, range_layers, operation_non_target=\"*1\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lape_langs = lape_langs_5\n\n\n#MODIFY  THIS ONLY \noperation_target = \"+10\"\nrange_layers=[5,17,18,19,20,21,22,23]\n#############\nq0 = free_text_eval(model, questions_answers[1:2], lape_langs, operation_target, range_layers, operation_non_target=\"*1\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lape_langs = lape_langs_5\n\n#MODIFY  THIS ONLY \noperation_target = \"+10\"\nrange_layers=[5,17,18,19,20,21,22,23]\n#############\nq0 = free_text_eval(model, questions_answers[2:3], lape_langs, operation_target, range_layers, operation_non_target=\"*1\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}